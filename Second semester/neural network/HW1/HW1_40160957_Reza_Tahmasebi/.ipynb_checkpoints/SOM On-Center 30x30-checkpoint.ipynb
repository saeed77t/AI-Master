{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18920917-d01c-4d7e-85b4-9f962c4e4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from minisom import MiniSom\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc4b21d-7b22-4514-b837-6f161ad6c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the root folder of your dataset\n",
    "root_folder = \"MNIST Dataset/\"\n",
    "\n",
    "# Define lists to store the image data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop over the folders in the root folder\n",
    "for i in range(10):\n",
    "    folder_path = root_folder + str(i) + \"/\"\n",
    "\n",
    "    # Get the label corresponding to the folder name\n",
    "    label = i\n",
    "\n",
    "    # Loop over the images in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Read the image using OpenCV and convert it to grayscale\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Add the image and label to the dataset lists\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert the data and labels lists to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets, with 90% of each class for training\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(10):\n",
    "    class_data = data[labels == i]\n",
    "    class_labels = labels[labels == i]\n",
    "\n",
    "    train_class_data, test_class_data, train_class_labels, test_class_labels = train_test_split(class_data, class_labels, test_size=0.1)\n",
    "\n",
    "    train_data.append(train_class_data)\n",
    "    train_labels.append(train_class_labels)\n",
    "    test_data.append(test_class_data)\n",
    "    test_labels.append(test_class_labels)\n",
    "\n",
    "# Concatenate the training and testing data and labels for each class\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "test_data = np.concatenate(test_data)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# Create a pandas dataframe to store the data and labels\n",
    "train_df = pd.DataFrame(train_data.reshape(train_data.shape[0], -1))\n",
    "train_df['label'] = train_labels\n",
    "\n",
    "test_df = pd.DataFrame(test_data.reshape(test_data.shape[0], -1))\n",
    "test_df['label'] = test_labels\n",
    "\n",
    "#shuffle data frame:\n",
    "train_df = train_df.sample(frac = 1)\n",
    "test_df = test_df.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d23f42-b237-4d42-ab9a-16344ba130d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Train and Test  Data and labels\n",
    "TrainLabels = train_df['label']\n",
    "TrainData = train_df.drop('label',axis=1)\n",
    "TestLabels = test_df['label']\n",
    "TestData = test_df.drop('label',axis=1)\n",
    "TrainData = np.array(TrainData)\n",
    "TrainLabels = np.array(TrainLabels)\n",
    "TestLabels = np.array(TestLabels)\n",
    "TestData = np.array(TestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56913a51-ce69-4276-8150-7d202a4e48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 384.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "done\n",
      "Davies-Bouldin index: 4.0326464254815\n",
      "Test data accuracy: 0.907\n",
      "Training data accuracy: 0.8818333333333334\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test data\n",
    "training_data = TrainData\n",
    "test_data = TestData\n",
    "training_labels = TrainLabels\n",
    "test_labels = TestLabels\n",
    "\n",
    "# Build SOM with 30x30 neurons\n",
    "som = MiniSom(30, 30, len(training_data[0]), sigma=1.0, learning_rate=0.5, neighborhood_function='bubble', topology='rectangular')\n",
    "\n",
    "# Train the SOM with all training set images\n",
    "som.random_weights_init(training_data)\n",
    "for i in tqdm(range(1000)):\n",
    "    som.train_random(training_data, num_iteration=1)\n",
    "\n",
    "# Get cluster labels for each training data point\n",
    "train_clusters = som.win_map(training_data)\n",
    "train_cluster_labels = []\n",
    "print(len(training_data))\n",
    "for i in range(len(training_data)):\n",
    "    cluster = train_clusters[(som.winner(training_data[i]))]\n",
    "    indices = [np.where(np.all(training_data == x, axis=1))[0][0] for x in cluster]\n",
    "    labels = [training_labels[index] for index in indices]\n",
    "    counts = np.bincount(labels)\n",
    "    train_cluster_labels.append(np.argmax(counts))\n",
    "    if(i % 500 == 0):\n",
    "        print(i)\n",
    "# print(\"Training data cluster labels:\", train_cluster_labels)\n",
    "\n",
    "# Get the number of images of each class in each cluster\n",
    "num_classes = len(np.unique(training_labels))\n",
    "cluster_class_counts = []\n",
    "for i in range(num_classes):\n",
    "    cluster_class_counts = []\n",
    "    print(i)\n",
    "    for j in range(30):\n",
    "        cluster = train_clusters[(i,j)]\n",
    "        counts = []\n",
    "        # print(j)\n",
    "        for k in range(len(training_data)):\n",
    "            if any(np.all(training_data[k] == x) for x in cluster):\n",
    "                indices = np.where(np.all(training_data == training_data[k], axis=1))[0]\n",
    "                if indices.size > 0:\n",
    "                    index = indices[0]\n",
    "                    counts.append(training_labels[index])\n",
    "        counts = np.bincount(counts, minlength=num_classes)\n",
    "        cluster_class_counts.append(counts)\n",
    "\n",
    "print('done')\n",
    "\n",
    "# Assign a label to each cluster based on the class with the highest count\n",
    "cluster_labels = []\n",
    "for counts in cluster_class_counts:\n",
    "    cluster_labels.append(np.argmax(counts))\n",
    "# print(\"Cluster labels:\", cluster_labels)\n",
    "\n",
    "# Compute Davies-Bouldin index on the clustering result\n",
    "dbi_score = davies_bouldin_score(training_data, train_cluster_labels)\n",
    "print(\"Davies-Bouldin index:\", dbi_score)\n",
    "\n",
    "# Get test data accuracy\n",
    "test_clusters = som.win_map(test_data)\n",
    "test_cluster_labels = []\n",
    "for i in range(len(test_data)):\n",
    "    cluster = test_clusters[(som.winner(test_data[i]))]\n",
    "    test_cluster_labels.append(np.argmax(np.bincount([test_labels[np.where(np.all(test_data == x, axis=1))[0][0]] for x in cluster if np.all(test_data == x, axis=1).any()])))\n",
    "\n",
    "test_accuracy = np.mean(np.array(test_cluster_labels) == np.array(test_labels))\n",
    "print(\"Test data accuracy:\", test_accuracy)\n",
    "\n",
    "# Get train data accuracy\n",
    "train_accuracy = np.mean(np.array(train_cluster_labels) == np.array(training_labels))\n",
    "print(\"Training data accuracy:\", train_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e019fda-40ad-4a87-a2cb-3c1fe7b59592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e4511c-cb16-4c3e-b63b-a520c69efb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0=[]\n",
    "ds1=[]\n",
    "ds2=[]\n",
    "ds3=[]\n",
    "ds4=[]\n",
    "ds5=[]\n",
    "ds6=[]\n",
    "ds7=[]\n",
    "ds8=[]\n",
    "ds9=[]\n",
    "lbl0=[]\n",
    "lbl1=[]\n",
    "lbl2=[]\n",
    "lbl3=[]\n",
    "lbl4=[]\n",
    "lbl5=[]\n",
    "lbl6=[]\n",
    "lbl7=[]\n",
    "lbl8=[]\n",
    "lbl9=[]\n",
    "\n",
    "poscnt = 0\n",
    "for i in range(len(training_data)):\n",
    "    if(training_labels[i]==0):\n",
    "        ds0.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl0.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl0.append(0)\n",
    "            \n",
    "    if(training_labels[i]==1):\n",
    "        ds1.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl1.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl1.append(0)\n",
    "            \n",
    "    if(training_labels[i]==2):\n",
    "        ds2.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl2.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl2.append(0)\n",
    "            \n",
    "    if(training_labels[i]==3):\n",
    "        ds3.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl3.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl3.append(0)\n",
    "\n",
    "    if(training_labels[i]==4):\n",
    "        ds4.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl4.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl4.append(0)\n",
    "            \n",
    "            \n",
    "    if(training_labels[i]==5):\n",
    "        ds5.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl5.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl5.append(0)\n",
    "            \n",
    "    if(training_labels[i]==6):\n",
    "        ds6.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl6.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl6.append(0)\n",
    "            \n",
    "    if(training_labels[i]==7):\n",
    "        ds7.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl7.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl7.append(0)\n",
    "            \n",
    "            \n",
    "    if(training_labels[i]==8):\n",
    "        ds8.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl8.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl8.append(0)\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(training_labels[i]==9):\n",
    "        ds9.append(training_data[i])\n",
    "        if(train_cluster_labels[i] == train_labels[i]):\n",
    "            lbl9.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl9.append(0)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05149951-57e8-4754-b901-f51730cb9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds0)\n",
    "df.to_excel('ds0.xlsx', index=False)\n",
    "df = pd.DataFrame(ds1)\n",
    "df.to_excel('ds1.xlsx', index=False)\n",
    "df = pd.DataFrame(ds2)\n",
    "df.to_excel('ds2.xlsx', index=False)\n",
    "df = pd.DataFrame(ds3)\n",
    "df.to_excel('ds3.xlsx', index=False)\n",
    "df = pd.DataFrame(ds4)\n",
    "df.to_excel('ds4.xlsx', index=False)\n",
    "df = pd.DataFrame(ds5)\n",
    "df.to_excel('ds5.xlsx', index=False)\n",
    "df = pd.DataFrame(ds6)\n",
    "df.to_excel('ds6.xlsx', index=False)\n",
    "df = pd.DataFrame(ds7)\n",
    "df.to_excel('ds7.xlsx', index=False)\n",
    "df = pd.DataFrame(ds8)\n",
    "df.to_excel('ds8.xlsx', index=False)\n",
    "df = pd.DataFrame(ds9)\n",
    "df.to_excel('ds9.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f2dd7c2-99b3-4f6c-944a-27a0ceefbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lbl0)\n",
    "df.to_excel('lbl0.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl1)\n",
    "df.to_excel('lbl1.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl2)\n",
    "df.to_excel('lbl2.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl3)\n",
    "df.to_excel('lbl3.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl4)\n",
    "df.to_excel('lbl4.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl5)\n",
    "df.to_excel('lbl5.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl6)\n",
    "df.to_excel('lbl6.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl7)\n",
    "df.to_excel('lbl7.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl8)\n",
    "df.to_excel('lbl8.xlsx', index=False)\n",
    "df = pd.DataFrame(lbl9)\n",
    "df.to_excel('lbl9.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2028b-ccc5-4e27-af12-0d5ccbe7b24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
