{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83275dac-2f09-456a-9979-411a1a567d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from minisom import MiniSom\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd172c8-627b-4e24-83df-3e303bdc4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the root folder of your dataset\n",
    "root_folder = \"MNIST Dataset/\"\n",
    "\n",
    "# Define lists to store the image data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop over the folders in the root folder\n",
    "for i in range(10):\n",
    "    folder_path = root_folder + str(i) + \"/\"\n",
    "\n",
    "    # Get the label corresponding to the folder name\n",
    "    label = i\n",
    "\n",
    "    # Loop over the images in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Read the image using OpenCV and convert it to grayscale\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Add the image and label to the dataset lists\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert the data and labels lists to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets, with 90% of each class for training\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(10):\n",
    "    class_data = data[labels == i]\n",
    "    class_labels = labels[labels == i]\n",
    "\n",
    "    train_class_data, test_class_data, train_class_labels, test_class_labels = train_test_split(class_data, class_labels, test_size=0.1)\n",
    "\n",
    "    train_data.append(train_class_data)\n",
    "    train_labels.append(train_class_labels)\n",
    "    test_data.append(test_class_data)\n",
    "    test_labels.append(test_class_labels)\n",
    "\n",
    "# Concatenate the training and testing data and labels for each class\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "test_data = np.concatenate(test_data)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# Create a pandas dataframe to store the data and labels\n",
    "train_df = pd.DataFrame(train_data.reshape(train_data.shape[0], -1))\n",
    "train_df['label'] = train_labels\n",
    "\n",
    "test_df = pd.DataFrame(test_data.reshape(test_data.shape[0], -1))\n",
    "test_df['label'] = test_labels\n",
    "\n",
    "#shuffle data frame:\n",
    "train_df = train_df.sample(frac = 1)\n",
    "test_df = test_df.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57997331-0fc1-45cc-a107-6b9870f4103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Train and Test  Data and labels\n",
    "TrainLabels = train_df['label']\n",
    "TrainData = train_df.drop('label',axis=1)\n",
    "TestLabels = test_df['label']\n",
    "TestData = test_df.drop('label',axis=1)\n",
    "TrainData = np.array(TrainData)\n",
    "TrainLabels = np.array(TrainLabels)\n",
    "TestLabels = np.array(TestLabels)\n",
    "TestData = np.array(TestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3497af02-ba0d-4c6e-9e66-b5e805f623e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 4.331078724100794\n",
      "dbi 3.935366642316235\n",
      "Train Accuracy for Winner-Takes-All approach: 0.9314444444444444\n",
      "Test Accuracy for Winner-Takes-All approach: 0.9215\n",
      "Cluster Labels: [[0. 0. 0. 0. 0. 0. 0. 1. 1. 2. 2. 2. 8. 6. 4. 4. 5. 5. 8. 8. 3. 3. 3. 3.\n",
      "  3. 3. 5. 5. 5. 0.]\n",
      " [0. 5. 0. 0. 6. 1. 1. 1. 1. 1. 7. 3. 3. 4. 4. 7. 5. 8. 8. 8. 3. 3. 5. 3.\n",
      "  5. 3. 3. 3. 5. 5.]\n",
      " [6. 5. 5. 3. 1. 1. 1. 1. 1. 1. 3. 7. 2. 2. 4. 9. 8. 8. 8. 8. 3. 3. 3. 3.\n",
      "  3. 3. 3. 3. 3. 3.]\n",
      " [7. 5. 1. 2. 1. 1. 1. 1. 1. 1. 7. 7. 7. 7. 7. 7. 9. 5. 8. 8. 3. 3. 3. 3.\n",
      "  3. 3. 3. 3. 3. 3.]\n",
      " [7. 2. 3. 1. 1. 1. 1. 1. 1. 1. 7. 7. 7. 7. 7. 7. 9. 3. 8. 8. 3. 3. 3. 3.\n",
      "  3. 3. 3. 3. 3. 3.]\n",
      " [2. 2. 2. 6. 2. 1. 1. 1. 1. 1. 7. 7. 7. 7. 7. 9. 9. 9. 8. 8. 3. 3. 3. 3.\n",
      "  3. 3. 3. 8. 8. 8.]\n",
      " [2. 4. 6. 6. 6. 1. 1. 1. 1. 1. 1. 8. 7. 9. 9. 9. 9. 9. 8. 8. 3. 3. 5. 5.\n",
      "  5. 5. 5. 5. 5. 5.]\n",
      " [5. 5. 6. 6. 6. 1. 1. 1. 1. 1. 1. 1. 8. 4. 9. 9. 9. 4. 4. 8. 3. 5. 5. 5.\n",
      "  5. 5. 5. 5. 5. 5.]\n",
      " [5. 6. 6. 6. 6. 1. 1. 1. 9. 1. 1. 1. 1. 4. 4. 4. 4. 4. 4. 5. 5. 5. 5. 5.\n",
      "  5. 5. 3. 3. 3. 8.]\n",
      " [5. 8. 6. 6. 7. 2. 1. 2. 2. 1. 1. 1. 1. 1. 7. 4. 4. 4. 4. 4. 5. 5. 5. 5.\n",
      "  5. 5. 3. 3. 3. 3.]\n",
      " [5. 0. 6. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 7. 9. 4. 4. 4. 4. 5. 5. 5. 5.\n",
      "  8. 8. 3. 3. 3. 8.]\n",
      " [6. 6. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 9. 7. 9. 9. 9. 9. 9. 9. 5. 5.\n",
      "  5. 8. 3. 3. 3. 8.]\n",
      " [6. 6. 4. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 4. 4. 9. 9. 9. 9. 9. 9. 9. 9.\n",
      "  8. 8. 8. 3. 8. 8.]\n",
      " [6. 6. 6. 6. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 8. 7. 2. 9. 4. 9. 9. 9. 9. 8.\n",
      "  8. 8. 8. 8. 8. 8.]\n",
      " [6. 6. 6. 6. 0. 4. 4. 2. 2. 2. 2. 8. 1. 1. 7. 7. 7. 9. 4. 4. 4. 9. 8. 8.\n",
      "  8. 8. 8. 8. 8. 8.]\n",
      " [6. 6. 6. 6. 4. 4. 4. 9. 7. 2. 3. 3. 3. 2. 2. 2. 2. 8. 4. 4. 4. 9. 8. 8.\n",
      "  8. 8. 8. 8. 8. 8.]\n",
      " [7. 7. 4. 4. 4. 4. 4. 9. 9. 5. 3. 3. 3. 3. 2. 2. 2. 2. 2. 4. 4. 4. 8. 8.\n",
      "  8. 8. 8. 8. 8. 8.]\n",
      " [9. 7. 4. 4. 4. 4. 4. 9. 9. 4. 3. 3. 3. 2. 2. 2. 2. 2. 2. 4. 4. 4. 8. 8.\n",
      "  8. 8. 8. 8. 8. 5.]\n",
      " [9. 9. 9. 4. 4. 4. 9. 9. 9. 9. 4. 3. 3. 2. 2. 2. 2. 2. 2. 2. 4. 5. 8. 8.\n",
      "  8. 8. 8. 5. 5. 5.]\n",
      " [4. 9. 9. 4. 4. 4. 4. 9. 9. 4. 4. 3. 3. 2. 2. 2. 2. 2. 2. 2. 6. 6. 6. 8.\n",
      "  6. 5. 5. 5. 5. 5.]\n",
      " [4. 9. 9. 7. 4. 4. 9. 9. 9. 9. 4. 3. 3. 8. 2. 2. 2. 8. 6. 6. 6. 6. 6. 6.\n",
      "  6. 6. 5. 5. 5. 5.]\n",
      " [9. 9. 9. 9. 9. 9. 9. 4. 4. 9. 4. 5. 5. 5. 8. 8. 6. 6. 6. 6. 6. 6. 6. 6.\n",
      "  6. 0. 0. 0. 0. 5.]\n",
      " [7. 7. 7. 7. 9. 4. 4. 4. 4. 4. 9. 0. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [7. 7. 7. 7. 9. 4. 4. 4. 9. 9. 9. 2. 5. 8. 5. 6. 6. 6. 6. 6. 6. 6. 6. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [7. 7. 7. 7. 7. 7. 9. 9. 9. 4. 2. 2. 3. 3. 8. 6. 6. 6. 6. 6. 6. 6. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [9. 9. 7. 7. 7. 7. 9. 4. 4. 4. 2. 2. 2. 2. 8. 6. 6. 6. 6. 6. 6. 6. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [9. 7. 7. 7. 7. 7. 7. 4. 4. 2. 2. 2. 2. 2. 8. 6. 6. 6. 6. 6. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [9. 7. 7. 7. 7. 7. 7. 7. 4. 2. 2. 2. 2. 2. 2. 3. 0. 5. 5. 5. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [9. 7. 7. 7. 7. 7. 7. 7. 2. 2. 2. 2. 2. 2. 2. 2. 8. 5. 5. 5. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [7. 7. 7. 7. 7. 7. 7. 7. 9. 2. 2. 2. 2. 2. 2. 2. 3. 5. 5. 5. 5. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#winner takes all approach , many clusters \n",
    "# Load the training and test data\n",
    "training_data = TrainData\n",
    "test_data = TestData\n",
    "training_labels = TrainLabels\n",
    "test_labels = TestLabels\n",
    "\n",
    "# Flatten the training and test data and normalize it:\n",
    "training_data = training_data.reshape(training_data.shape[0], -1) / 255.\n",
    "test_data = test_data.reshape(test_data.shape[0], -1) / 255.\n",
    "\n",
    "# Define the parameters for the SOM:\n",
    "input_len = 784  # number of features\n",
    "classes = 10     # number of classes\n",
    "som_size = 30    # size of the SOM\n",
    "sigma = 1.0      # neighborhood radius\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Implement the Winner-Takes-All approach:\n",
    "# Create a SOM with one neuron for each class\n",
    "som = MiniSom(som_size, som_size, input_len, sigma=sigma, learning_rate=learning_rate, \n",
    "              neighborhood_function='gaussian', random_seed=0)\n",
    "som.pca_weights_init(training_data)\n",
    "som.train_batch(training_data, 100000, verbose=True)\n",
    "\n",
    "# Determine the number of images of each class that are placed in each cluster\n",
    "labels_map = som.labels_map(training_data, training_labels)\n",
    "class_counts = np.zeros((som_size, som_size, classes))\n",
    "for index, label in np.ndenumerate(training_labels):\n",
    "    i, j = som.winner(training_data[index])\n",
    "    class_counts[i, j, label] += 1\n",
    "\n",
    "# Determine the label for each cluster\n",
    "class_labels = np.zeros((som_size, som_size))\n",
    "for i in range(som_size):\n",
    "    for j in range(som_size):\n",
    "        class_labels[i, j] = np.argmax(class_counts[i, j])\n",
    "\n",
    "# Compute the DBI evaluation criteria on the clustering result\n",
    "labels_map = som.labels_map(training_data, training_labels)\n",
    "# dbi_wta = davies_bouldin_score(training_data, np.array([class_labels[x] for x in labels_map]))\n",
    "dbi_wta = davies_bouldin_score(training_data, training_labels)\n",
    "print('dbi',dbi_wta)\n",
    "# Compute the train data accuracy\n",
    "train_labels_pred = [np.argmax(class_counts[som.winner(x)]) for x in training_data]\n",
    "accuracy_wta_train = np.mean(np.equal(training_labels, train_labels_pred))\n",
    "print('Train Accuracy for Winner-Takes-All approach:', accuracy_wta_train)\n",
    "\n",
    "# Compute the test data accuracy\n",
    "test_labels_pred = [np.argmax(class_counts[som.winner(x)]) for x in test_data]\n",
    "accuracy_wta_test = np.mean(np.equal(test_labels, test_labels_pred))\n",
    "print('Test Accuracy for Winner-Takes-All approach:', accuracy_wta_test)\n",
    "\n",
    "print(\"Cluster Labels:\",class_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad72ba02-fc9b-4163-af8f-7ab3755824d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0=[]\n",
    "ds1=[]\n",
    "ds2=[]\n",
    "ds3=[]\n",
    "ds4=[]\n",
    "ds5=[]\n",
    "ds6=[]\n",
    "ds7=[]\n",
    "ds8=[]\n",
    "ds9=[]\n",
    "lbl0=[]\n",
    "lbl1=[]\n",
    "lbl2=[]\n",
    "lbl3=[]\n",
    "lbl4=[]\n",
    "lbl5=[]\n",
    "lbl6=[]\n",
    "lbl7=[]\n",
    "lbl8=[]\n",
    "lbl9=[]\n",
    "\n",
    "poscnt = 0\n",
    "for i in range(len(training_data)):\n",
    "    if(training_labels[i]==0):\n",
    "        ds0.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl0.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl0.append(0)\n",
    "            \n",
    "    if(training_labels[i]==1):\n",
    "        ds1.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl1.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl1.append(0)\n",
    "            \n",
    "    if(training_labels[i]==2):\n",
    "        ds2.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl2.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl2.append(0)\n",
    "            \n",
    "    if(training_labels[i]==3):\n",
    "        ds3.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl3.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl3.append(0)\n",
    "\n",
    "    if(training_labels[i]==4):\n",
    "        ds4.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl4.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl4.append(0)\n",
    "            \n",
    "            \n",
    "    if(training_labels[i]==5):\n",
    "        ds5.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl5.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl5.append(0)\n",
    "            \n",
    "    if(training_labels[i]==6):\n",
    "        ds6.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl6.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl6.append(0)\n",
    "            \n",
    "    if(training_labels[i]==7):\n",
    "        ds7.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl7.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl7.append(0)\n",
    "            \n",
    "            \n",
    "    if(training_labels[i]==8):\n",
    "        ds8.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl8.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl8.append(0)\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(training_labels[i]==9):\n",
    "        ds9.append(training_data[i])\n",
    "        if(train_labels_pred[i] == train_labels[i]):\n",
    "            lbl9.append(1)\n",
    "            poscnt +=1\n",
    "        else:\n",
    "            lbl9.append(0)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96747822-0a56-4c90-aae2-c4f36450f17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564b480-38e4-472c-8659-e34d4a1ced55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b62f7d-fbcd-47f6-aa8b-558ff01be438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ec548-f3f2-4a21-becb-660f377074ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
