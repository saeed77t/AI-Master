{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44e75e3-ef8c-480d-96e1-012ae792c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 1s 6ms/step - loss: 1.6074 - accuracy: 0.2337 - val_loss: 1.5967 - val_accuracy: 0.3308\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 1.5654 - accuracy: 0.3303 - val_loss: 1.5492 - val_accuracy: 0.4808\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 1.4170 - accuracy: 0.4837 - val_loss: 1.3619 - val_accuracy: 0.6154\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 1.1065 - accuracy: 0.6264 - val_loss: 1.0847 - val_accuracy: 0.7077\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.7874 - accuracy: 0.7423 - val_loss: 0.8697 - val_accuracy: 0.7442\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.8231 - val_loss: 0.7451 - val_accuracy: 0.7769\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.8913 - val_loss: 0.6775 - val_accuracy: 0.7769\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.9192 - val_loss: 0.6383 - val_accuracy: 0.7904\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9317 - val_loss: 0.6153 - val_accuracy: 0.7846\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.1961 - accuracy: 0.9476 - val_loss: 0.6206 - val_accuracy: 0.7942\n",
      "Epoch 11/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9558 - val_loss: 0.6270 - val_accuracy: 0.7846\n",
      "Epoch 12/20\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.1473 - accuracy: 0.9572 - val_loss: 0.6282 - val_accuracy: 0.7904\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.7483 - accuracy: 0.7415\n",
      "Train Loss: 0.0104924151673913\n",
      "Train Accuracy: 1.0\n",
      "Test Loss: 0.7483240962028503\n",
      "Test Accuracy: 0.7415384650230408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def preprocess_text(text, stop_words):\n",
    "    # Remove special characters and lowercase the text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    # Remove stopwords\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=vocab_size))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
    "    print('X_train shape :', X_train.shape)\n",
    "    print('X_test shape :' ,X_val.shape)\n",
    "    print('y_train shape :' ,y_val.shape)\n",
    "    max\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(\"Train Loss:\", train_loss)\n",
    "    print(\"Train Accuracy:\", train_acc)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "def text_emotion_detection(dataset_path, stopwords_path):\n",
    "    # Step 1: Load the dataset\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Step 2: Load stopwords\n",
    "    stop_words = set(open(stopwords_path, 'r').read().split())\n",
    "\n",
    "    # Step 3: Preprocess the text\n",
    "    data['Text'] = data['Text'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "    # Step 4: Split the dataset into train, validation, and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 5: Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['Text'])\n",
    "    train_data_encoded = tokenizer.texts_to_matrix(train_data['Text'], mode='binary')\n",
    "    val_data_encoded = tokenizer.texts_to_matrix(val_data['Text'], mode='binary')\n",
    "    test_data_encoded = tokenizer.texts_to_matrix(test_data['Text'], mode='binary')\n",
    "\n",
    "    # Step 6: Encode emotion labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_data['Label'])\n",
    "    train_labels_encoded = label_encoder.transform(train_data['Label'])\n",
    "    val_labels_encoded = label_encoder.transform(val_data['Label'])\n",
    "    test_labels_encoded = label_encoder.transform(test_data['Label'])\n",
    "\n",
    "    # Step 7: Build the model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = build_model(vocab_size)\n",
    "\n",
    "    # Step 8: Train the model\n",
    "    train_model(model, train_data_encoded, train_labels_encoded, val_data_encoded, val_labels_encoded)\n",
    "\n",
    "    # Step 9: Evaluate the model\n",
    "    evaluate_model(model, train_data_encoded, train_labels_encoded, test_data_encoded, test_labels_encoded)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'Dataset/Text_Emotion_Data.csv'\n",
    "stopwords_path = 'Dataset/stopwords.txt'\n",
    "text_emotion_detection(dataset_path, stopwords_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592d73c-c100-41e5-8a74-21e3074614e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
