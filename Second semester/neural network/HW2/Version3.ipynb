{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85582d-2a11-4b57-b592-514a57d86a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee886c-9dfe-4a92-9ffd-5d9d796945dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfeedae-ab4d-44fd-99ad-14d820a73dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset/Text_Emotion_Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    117\u001b[0m stopwords_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset/stopwords.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtext_emotion_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtext_emotion_detection\u001b[0;34m(dataset_path, stopwords_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_text(x, stop_words))\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Step 4: Split the dataset into train, validation, and test sets\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#     train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \n\u001b[1;32m     66\u001b[0m     \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Split last 150 text of each class for the test dataset\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     69\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "\n",
    "\n",
    "def preprocess_text(text, stop_words):\n",
    "    # Remove special characters and lowercase the text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    # Remove stopwords\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, activation='relu', input_dim=( 6609,2080)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
    "    print('X_train shape :', X_train.shape)\n",
    "    print('X_test shape :' ,X_val.shape)\n",
    "    print('y_train shape :' ,y_val.shape)\n",
    "    print(y_train)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(\"Train Loss:\", train_loss)\n",
    "    print(\"Train Accuracy:\", train_acc)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "    print(X_train)\n",
    "\n",
    "\n",
    "def text_emotion_detection(dataset_path, stopwords_path):\n",
    "    # Step 1: Load the dataset\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Step 2: Load stopwords\n",
    "    stop_words = set(open(stopwords_path, 'r').read().split())\n",
    "\n",
    "    # Step 3: Preprocess the text\n",
    "    data['Text'] = data['Text'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "    # Step 4: Split the dataset into train, validation, and test sets\n",
    "#     train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "#     train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    # Split last 150 text of each class for the test dataset\n",
    "    classes = np.unique.data['Label']\n",
    "    test_data = []\n",
    "    for c in classes:\n",
    "        class_data = [(X[i], y[i]) for i in range(len(X)) if y[i] == label_map[c]]\n",
    "        test_data.extend(class_data[-150:])\n",
    "\n",
    "    # Use the rest of the data for training\n",
    "    train_data = []\n",
    "    for i in range(len(X)):\n",
    "        found = False\n",
    "        for j in range(len(test_data)):\n",
    "            if all(X[i] == test_data[j][0]) and y[i] == test_data[j][1]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            train_data.append((X[i], y[i]))\n",
    "\n",
    "    # Separate the input features and labels for the training and test sets\n",
    "    X_train, y_train = zip(*train_data)\n",
    "    X_test, y_test = zip(*test_data)\n",
    "\n",
    "    # Step 5: Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['Text'])\n",
    "    train_data_encoded = tokenizer.texts_to_matrix(train_data['Text'], mode='binary')\n",
    "    val_data_encoded = tokenizer.texts_to_matrix(val_data['Text'], mode='binary')\n",
    "    test_data_encoded = tokenizer.texts_to_matrix(test_data['Text'], mode='binary')\n",
    "\n",
    "    # Step 6: Encode emotion labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_data['Label'])\n",
    "    train_labels_encoded = label_encoder.transform(train_data['Label'])\n",
    "    val_labels_encoded = label_encoder.transform(val_data['Label'])\n",
    "    test_labels_encoded = label_encoder.transform(test_data['Label'])\n",
    "\n",
    "    # Step 7: Build the model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    model = build_model(vocab_size)\n",
    "\n",
    "    # Step 8: Train the model\n",
    "    train_model(model, train_data_encoded, train_labels_encoded, val_data_encoded, val_labels_encoded)\n",
    "\n",
    "    # Step 9: Evaluate the model\n",
    "    evaluate_model(model, train_data_encoded, train_labels_encoded, test_data_encoded, test_labels_encoded)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'Dataset/Text_Emotion_Data.csv'\n",
    "stopwords_path = 'Dataset/stopwords.txt'\n",
    "text_emotion_detection(dataset_path, stopwords_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65de21-d6e9-40cf-91ac-be819d06f615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
