{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN\nfrom keras.layers import Dropout\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:51:55.787189Z","iopub.execute_input":"2023-05-10T08:51:55.787911Z","iopub.status.idle":"2023-05-10T08:51:55.794004Z","shell.execute_reply.started":"2023-05-10T08:51:55.787869Z","shell.execute_reply":"2023-05-10T08:51:55.792885Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"classes = np.unique(df['Label'])\nprint('labels = ',classes)\n\nlabel = np.array(df['Label'])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:44:06.273459Z","iopub.execute_input":"2023-05-10T08:44:06.273849Z","iopub.status.idle":"2023-05-10T08:44:06.282581Z","shell.execute_reply.started":"2023-05-10T08:44:06.273819Z","shell.execute_reply":"2023-05-10T08:44:06.281493Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"labels =  ['anger' 'fear' 'joy' 'love' 'sadness']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read the Excel file into a pandas DataFrame\ndf = pd.read_csv('/kaggle/input/nn-hw2/Dataset/Text_Emotion_Data.csv')\npattern = r'[^a-zA-Z]'\n\n# Define a function to tokenize text into word sequences and remove stopwords\ndef tokenize_text(text):\n    # Remove non-letter characters using the defined pattern\n    text = re.sub(pattern, ' ', text)\n    # Convert the text to lowercase\n    text = text.lower()\n    # Tokenize the text into word sequences\n    word_sequences = text.split()\n    # Remove stopwords using the provided list\n    with open('/kaggle/input/nn-hw2/Dataset/stopwords.txt', 'r') as f:\n        stopwords = f.read().splitlines()\n    # Remove words with length less than or equal to 2\n    word_sequences = [word for word in word_sequences if (word not in stopwords and len(word) > 2)]\n    return word_sequences\n\n# Tokenize each row of the text column into word sequences using the defined function\ndf['word_sequences'] = df['Text'].apply(tokenize_text)\n\n# Find the maximum length of a word sequence\nmax_len = max(df['word_sequences'].apply(len))\n\n# Define a function to pad the sequences to the maximum length\ndef pad_sequence(sequence):\n    padded_sequence = sequence[:max_len] + ['']*(max_len-len(sequence))\n    return padded_sequence\n\n# Pad each sequence to the maximum length\ndf['word_sequences'] = df['word_sequences'].apply(pad_sequence)\n\n\n\n# # Define the label mapping\n# label_map = {'anger': 0, 'fear': 1, 'joy': 2, 'love': 3, 'sadness': 4}\n# # Replace the labels with their corresponding numeric values\n# df['Label'] = df['Label'].replace(label_map)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:39:28.061381Z","iopub.execute_input":"2023-05-10T08:39:28.061781Z","iopub.status.idle":"2023-05-10T08:39:29.731964Z","shell.execute_reply.started":"2023-05-10T08:39:28.061748Z","shell.execute_reply":"2023-05-10T08:39:29.731189Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# new mwthod :\n# Combine all word sequences into a single list\nall_sequences = []\nfor seq in df['word_sequences']:\n    all_sequences.append(seq)\n\n# Create a dictionary with unique words as keys and their corresponding index as values\nword_dict = {}\nindex = 0\nfor seq in all_sequences:\n    for word in seq:\n        if word not in word_dict:\n            word_dict[word] = index\n            index += 1\n\n# Convert each word sequence into a numerical vector with the corresponding index in the dictionary\nnum_vectors = []\nfor sequence in all_sequences:\n    vector = []\n    for word in sequence:\n        if word in word_dict:\n            index = word_dict[word]\n            vector.append(index)\n    num_vectors.append(vector)\n\n# Convert the list of numerical vectors into a numpy array\nX = np.array(num_vectors)\n\n# Print the shape of the resulting numerical vectors\nprint('Shape of numerical vectors:', X.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:39:29.771897Z","iopub.execute_input":"2023-05-10T08:39:29.772415Z","iopub.status.idle":"2023-05-10T08:39:29.854391Z","shell.execute_reply.started":"2023-05-10T08:39:29.772387Z","shell.execute_reply":"2023-05-10T08:39:29.853253Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Shape of numerical vectors: (3250, 27)\n","output_type":"stream"}]},{"cell_type":"code","source":"y = label\n# Split last 150 text of each class for the test dataset\ntest_data = []\nfor c in classes:\n    class_data = [(X[i], y[i]) for i in range(len(X)) if y[i] == c]\n    test_data.extend(class_data[-150:])\n\n# Use the rest of the data for training\ntrain_data = []\nfor i in range(len(X)):\n    found = False\n    for j in range(len(test_data)):\n        if all(X[i] == test_data[j][0]) and y[i] == test_data[j][1]:\n            found = True\n            break\n    if not found:\n        train_data.append((X[i], y[i]))\n\n# Separate the input features and labels for the training and test sets\nX_train, y_train = zip(*train_data)\nX_test, y_test = zip(*test_data)\n\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:39:31.241841Z","iopub.execute_input":"2023-05-10T08:39:31.242508Z","iopub.status.idle":"2023-05-10T08:39:35.085844Z","shell.execute_reply.started":"2023-05-10T08:39:31.242472Z","shell.execute_reply":"2023-05-10T08:39:35.084616Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Instantiate the label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit the label encoder to the class labels\nlabel_encoder.fit(classes)\n\n# Convert the class labels to integer values\ny_train = label_encoder.transform(y_train)\ny_test = label_encoder.transform(y_test)\n\n# Convert the integer class labels to one-hot encoded vectors\ny_train = to_categorical(y_train, num_classes=len(classes))\ny_test = to_categorical(y_test, num_classes=len(classes))\n\n\nX_train = X_train.reshape(X_train.shape[0], max_len, len(word_dict))\nX_test = X_test.reshape(X_test.shape[0], max_len, len(word_dict))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:40:36.564304Z","iopub.execute_input":"2023-05-10T08:40:36.564682Z","iopub.status.idle":"2023-05-10T08:40:36.603609Z","shell.execute_reply.started":"2023-05-10T08:40:36.564653Z","shell.execute_reply":"2023-05-10T08:40:36.602172Z"},"trusted":true},"execution_count":58,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m y_train \u001b[38;5;241m=\u001b[39m to_categorical(y_train, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes))\n\u001b[1;32m     15\u001b[0m y_test \u001b[38;5;241m=\u001b[39m to_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes))\n\u001b[0;32m---> 18\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], max_len, \u001b[38;5;28mlen\u001b[39m(word_dict))\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 67500 into shape (2500,27,6623)"],"ename":"ValueError","evalue":"cannot reshape array of size 67500 into shape (2500,27,6623)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Build the model architecture\nmodel = Sequential()\nmodel.add(SimpleRNN(units=64, input_shape=(max_len, len(word_dict)), activation='tanh'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=len(classes), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32)\n\n# Evaluate the model on the train dataset\ntrain_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Accuracy:', train_acc)\n\n# Evaluate the model on the test dataset\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Accuracy:', test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:32:39.556134Z","iopub.execute_input":"2023-05-10T08:32:39.556512Z","iopub.status.idle":"2023-05-10T08:32:39.780882Z","shell.execute_reply.started":"2023-05-10T08:32:39.556484Z","shell.execute_reply":"2023-05-10T08:32:39.778965Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the train dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, y_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_file86ukwws4.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 27, 6623), found shape=(None, 1, 27)\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 27, 6623), found shape=(None, 1, 27)\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# Read the Excel file into a pandas DataFrame\ndf = pd.read_csv('/kaggle/input/nn-hw2/Dataset/Text_Emotion_Data.csv')\n\n# Define a function to tokenize text into word sequences and remove stopwords\ndef tokenize_text(text):\n    # Remove non-letter characters using the defined pattern\n    pattern = r'[^a-zA-Z]'\n    text = re.sub(pattern, ' ', text)\n    # Convert the text to lowercase\n    text = text.lower()\n    # Tokenize the text into word sequences\n    word_sequences = text.split()\n    # Remove stopwords using the provided list\n    with open('/kaggle/input/nn-hw2/Dataset/stopwords.txt', 'r') as f:\n        stopwords = f.read().splitlines()\n    # Remove words with length less than or equal to 2\n    word_sequences = [word for word in word_sequences if (word not in stopwords and len(word) > 2)]\n    return word_sequences\n\n# Tokenize each row of the text column into word sequences using the defined function\ndf['word_sequences'] = df['Text'].apply(tokenize_text)\n\n# Find the maximum length of a word sequence\nmax_len = max(df['word_sequences'].apply(len))\n\n# Define a function to pad the sequences to the maximum length\ndef pad_sequence(sequence):\n    padded_sequence = sequence[:max_len] + ['']*(max_len-len(sequence))\n    return padded_sequence\n\n# Pad each sequence to the maximum length\ndf['word_sequences'] = df['word_sequences'].apply(pad_sequence)\n\n# Combine all word sequences into a single list\nall_sequences = []\nfor seq in df['word_sequences']:\n    all_sequences.append(seq)\n\n# Create a dictionary with unique words as keys and their corresponding index as values\nword_dict = {}\nindex = 0\nfor seq in all_sequences:\n    for word in seq:\n        if word not in word_dict:\n            word_dict[word] = index\n            index += 1\n\n# Convert each word sequence into a numerical vector with the corresponding index in the dictionary\nnum_vectors = []\nfor sequence in all_sequences:\n    vector = []\n    for word in sequence:\n        if word in word_dict:\n            index = word_dict[word]\n            vector.append(index)\n    num_vectors.append(vector)\n\n# Convert the list of numerical vectors into a numpy array\nX = np.array(num_vectors)\n\n# Define the labels\nlabels = df['Label'].values\nclasses = np.unique(labels)\nlabel_map = {label: i for i, label in enumerate(classes)}\ny = np.array([label_map[label] for label in labels])\n\n# Split last 150 text of each class for the test dataset\ntest_data = []\nfor c in classes:\n    class_data = [(X[i], y[i]) for i in range(len(X)) if y[i] == label_map[c]]\n    test_data.extend(class_data[-150:])\n\n# Use the rest of the data for training\ntrain_data = []\nfor i in range(len(X)):\n    found = False\n    for j in range(len(test_data)):\n        if all(X[i] == test_data[j][0]) and y[i] == test_data[j][1]:\n            found = True\n            break\n    if not found:\n        train_data.append((X[i], y[i]))\n\n# Separate the input features and labels for the training and test sets\nX_train, y_train = zip(*train_data)\nX_test, y_test = zip(*test_data)\n\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:58:14.961909Z","iopub.execute_input":"2023-05-10T08:58:14.962914Z","iopub.status.idle":"2023-05-10T08:58:20.417849Z","shell.execute_reply.started":"2023-05-10T08:58:14.962867Z","shell.execute_reply":"2023-05-10T08:58:20.416840Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"print('X_train shape :', X_train.shape)\nprint('X_test shape :' ,X_test.shape)\nprint('y_train shape :' ,y_train.shape)\nprint('y_test shape :', y_test.shape)\nprint('word_dict len :', len(word_dict))\nprint('max_len  :' ,max_len) \nX_train[20]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:59:52.424763Z","iopub.execute_input":"2023-05-10T08:59:52.425199Z","iopub.status.idle":"2023-05-10T08:59:52.435049Z","shell.execute_reply.started":"2023-05-10T08:59:52.425164Z","shell.execute_reply":"2023-05-10T08:59:52.433857Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"X_train shape : (2500, 27, 1)\nX_test shape : (750, 27, 1)\ny_train shape : (2500, 5)\ny_test shape : (750, 5)\nword_dict len : 6623\nmax_len  : 27\n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"array([[203],\n       [ 65],\n       [204],\n       [ 10],\n       [205],\n       [  2],\n       [ 19],\n       [206],\n       [207],\n       [133],\n       [208],\n       [176],\n       [209],\n       [210],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15],\n       [ 15]])"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Reshape the input data to have a third dimension\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n# Convert y_train to one-hot encoding\ny_train = to_categorical(y_train, num_classes=5)\ny_test= to_categorical(y_test, num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:58:20.427870Z","iopub.execute_input":"2023-05-10T08:58:20.428547Z","iopub.status.idle":"2023-05-10T08:58:20.448776Z","shell.execute_reply.started":"2023-05-10T08:58:20.428503Z","shell.execute_reply":"2023-05-10T08:58:20.447559Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\nmodel = Sequential()\nmodel.add(SimpleRNN(64, input_shape=(max_len, 1), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(SimpleRNN(64))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(classes), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=1000, batch_size=32)\n\n# Evaluate the model on the train dataset\ntrain_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Loss:', train_loss)\nprint('Train Accuracy:', train_acc)\n\n# Evaluate the model on the test dataset\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T09:00:03.401099Z","iopub.execute_input":"2023-05-10T09:00:03.401550Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n79/79 [==============================] - 3s 13ms/step - loss: 1.7866 - accuracy: 0.1940\nEpoch 2/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.7624 - accuracy: 0.1924\nEpoch 3/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.7319 - accuracy: 0.2064\nEpoch 4/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.7285 - accuracy: 0.1940\nEpoch 5/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.7154 - accuracy: 0.1968\nEpoch 6/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6894 - accuracy: 0.2216\nEpoch 7/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6883 - accuracy: 0.2056\nEpoch 8/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6874 - accuracy: 0.1912\nEpoch 9/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6844 - accuracy: 0.1956\nEpoch 10/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6863 - accuracy: 0.1924\nEpoch 11/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6851 - accuracy: 0.1952\nEpoch 12/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6637 - accuracy: 0.1960\nEpoch 13/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6597 - accuracy: 0.2080\nEpoch 14/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6441 - accuracy: 0.1984\nEpoch 15/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6535 - accuracy: 0.1984\nEpoch 16/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6427 - accuracy: 0.2068\nEpoch 17/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6468 - accuracy: 0.1976\nEpoch 18/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6417 - accuracy: 0.2028\nEpoch 19/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6312 - accuracy: 0.2116\nEpoch 20/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6246 - accuracy: 0.2124\nEpoch 21/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6305 - accuracy: 0.2024\nEpoch 22/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6365 - accuracy: 0.1832\nEpoch 23/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6269 - accuracy: 0.2004\nEpoch 24/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6274 - accuracy: 0.2024\nEpoch 25/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6198 - accuracy: 0.2184\nEpoch 26/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6179 - accuracy: 0.2172\nEpoch 27/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6231 - accuracy: 0.2136\nEpoch 28/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6220 - accuracy: 0.2064\nEpoch 29/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6176 - accuracy: 0.2124\nEpoch 30/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6174 - accuracy: 0.2176\nEpoch 31/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6216 - accuracy: 0.2020\nEpoch 32/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6130 - accuracy: 0.2204\nEpoch 33/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6142 - accuracy: 0.2260\nEpoch 34/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6160 - accuracy: 0.2072\nEpoch 35/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6142 - accuracy: 0.2152\nEpoch 36/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6140 - accuracy: 0.1996\nEpoch 37/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6089 - accuracy: 0.2184\nEpoch 38/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6151 - accuracy: 0.2060\nEpoch 39/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6133 - accuracy: 0.2072\nEpoch 40/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6093 - accuracy: 0.2276\nEpoch 41/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6115 - accuracy: 0.2180\nEpoch 42/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6098 - accuracy: 0.2160\nEpoch 43/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6099 - accuracy: 0.2232\nEpoch 44/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6126 - accuracy: 0.2192\nEpoch 45/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6097 - accuracy: 0.2204\nEpoch 46/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6092 - accuracy: 0.2188\nEpoch 47/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6148 - accuracy: 0.2028\nEpoch 48/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6090 - accuracy: 0.2184\nEpoch 49/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6100 - accuracy: 0.2224\nEpoch 50/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6112 - accuracy: 0.2100\nEpoch 51/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6072 - accuracy: 0.2256\nEpoch 52/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6060 - accuracy: 0.2280\nEpoch 53/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6078 - accuracy: 0.2128\nEpoch 54/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6090 - accuracy: 0.2164\nEpoch 55/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6087 - accuracy: 0.2096\nEpoch 56/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6071 - accuracy: 0.2208\nEpoch 57/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6069 - accuracy: 0.2328\nEpoch 58/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6069 - accuracy: 0.2192\nEpoch 59/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6039 - accuracy: 0.2332\nEpoch 60/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6079 - accuracy: 0.2260\nEpoch 61/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6065 - accuracy: 0.2284\nEpoch 62/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6039 - accuracy: 0.2288\nEpoch 63/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6028 - accuracy: 0.2288\nEpoch 64/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6064 - accuracy: 0.2280\nEpoch 65/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6064 - accuracy: 0.2324\nEpoch 66/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6067 - accuracy: 0.2220\nEpoch 67/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6042 - accuracy: 0.2432\nEpoch 68/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6060 - accuracy: 0.2236\nEpoch 69/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6052 - accuracy: 0.2228\nEpoch 70/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6032 - accuracy: 0.2284\nEpoch 71/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6044 - accuracy: 0.2320\nEpoch 72/1000\n79/79 [==============================] - 1s 15ms/step - loss: 1.6026 - accuracy: 0.2368\nEpoch 73/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6051 - accuracy: 0.2300\nEpoch 74/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6004 - accuracy: 0.2388\nEpoch 75/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6089 - accuracy: 0.2188\nEpoch 76/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6070 - accuracy: 0.2260\nEpoch 77/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6042 - accuracy: 0.2424\nEpoch 78/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6046 - accuracy: 0.2352\nEpoch 79/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6063 - accuracy: 0.2272\nEpoch 80/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6035 - accuracy: 0.2372\nEpoch 81/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6057 - accuracy: 0.2324\nEpoch 82/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6072 - accuracy: 0.2232\nEpoch 83/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6074 - accuracy: 0.2248\nEpoch 84/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6066 - accuracy: 0.2236\nEpoch 85/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6076 - accuracy: 0.2228\nEpoch 86/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6064 - accuracy: 0.2244\nEpoch 87/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6051 - accuracy: 0.2232\nEpoch 88/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6056 - accuracy: 0.2332\nEpoch 89/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6058 - accuracy: 0.2344\nEpoch 90/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6053 - accuracy: 0.2396\nEpoch 91/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6044 - accuracy: 0.2336\nEpoch 92/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6041 - accuracy: 0.2308\nEpoch 93/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6064 - accuracy: 0.2272\nEpoch 94/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6043 - accuracy: 0.2372\nEpoch 95/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6036 - accuracy: 0.2396\nEpoch 96/1000\n79/79 [==============================] - 1s 16ms/step - loss: 1.6040 - accuracy: 0.2300\nEpoch 97/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6032 - accuracy: 0.2332\nEpoch 98/1000\n79/79 [==============================] - 1s 14ms/step - loss: 1.6037 - accuracy: 0.2380\nEpoch 99/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6023 - accuracy: 0.2412\nEpoch 100/1000\n79/79 [==============================] - 1s 13ms/step - loss: 1.6045 - accuracy: 0.2340\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}