{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c37d578-998f-494c-a886-bd776c0e2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import gym\n",
    "\n",
    "\n",
    "class PacManRL:\n",
    "    def __init__(self, num_episodes=1000, gamma=0.99, alpha=0.5, epsilon=1.0, epsilon_decay=0.01, epsilon_min=0.01):\n",
    "        self.q_table = {}\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # Define the action space and initialize the Q-table\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.actions)))\n",
    "\n",
    "        # Define features for state representation\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "        \n",
    "        \n",
    "    def get_state(self, game_state):\n",
    "        # Extract relevant information from the game state to represent it as a state\n",
    "        if isinstance(game_state, dict):\n",
    "            pacman_pos = game_state.get('pacman_pos', ())\n",
    "            food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "            ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "        else:\n",
    "            pacman_pos = ()\n",
    "            food_pos = ()\n",
    "            ghost_pos = ()\n",
    "\n",
    "        # Compute the remaining food left\n",
    "        if self.food_left is None:\n",
    "            self.food_left = len(food_pos)\n",
    "        else:\n",
    "            self.food_left -= self.food_eaten\n",
    "\n",
    "        # Compute the distance to the nearest food and the nearest ghost\n",
    "        dist_to_food = np.inf\n",
    "        for pos in food_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_food:\n",
    "                dist_to_food = dist\n",
    "\n",
    "        dist_to_ghost = np.inf\n",
    "        for pos in ghost_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_ghost:\n",
    "                dist_to_ghost = dist\n",
    "\n",
    "        # Compute the new features\n",
    "        self.food_eaten = self.food_left - len(food_pos)\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'food_distance': dist_to_food,\n",
    "            'ghost_distance': dist_to_ghost\n",
    "        }\n",
    "\n",
    "        return {'pacman_pos': pacman_pos, 'food_pos': food_pos, 'ghost_pos': ghost_pos}\n",
    "\n",
    "\n",
    "\n",
    "#     def get_state(self, game_state):\n",
    "#         # Extract relevant information from the game state to represent it as a state\n",
    "#         pacman_pos = tuple(game_state['pacman_pos'])\n",
    "#         food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "#         ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "\n",
    "#         # Compute the remaining food left\n",
    "#         if self.food_left is None:\n",
    "#             self.food_left = len(food_pos)\n",
    "#         else:\n",
    "#             self.food_left -= self.food_eaten\n",
    "\n",
    "#         # Compute the distance to the nearest food and the nearest ghost\n",
    "#         dist_to_food = np.inf\n",
    "#         for pos in food_pos:\n",
    "#             dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "#             if dist < dist_to_food:\n",
    "#                 dist_to_food = dist\n",
    "\n",
    "#         dist_to_ghost = np.inf\n",
    "#         for pos in ghost_pos:\n",
    "#             dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "#             if dist < dist_to_ghost:\n",
    "#                 dist_to_ghost = dist\n",
    "\n",
    "#         # Compute the new features\n",
    "#         self.food_eaten = self.food_left - len(food_pos)\n",
    "#         features = {\n",
    "#             'bias': 1.0,\n",
    "#             'food_distance': dist_to_food,\n",
    "#             'ghost_distance': dist_to_ghost\n",
    "#         }\n",
    "\n",
    "#         pacman_pos = tuple(game_state.get('pacman_pos', ()))\n",
    "#         food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "#         ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "#         return {'pacman_pos': pacman_pos, 'food_pos': food_pos, 'ghost_pos': ghost_pos}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = self.q_table.get(state, {a: 0 for a in self.actions})\n",
    "            max_q_value = max(q_values.values())\n",
    "            actions_with_max_q_value = [a for a, q in q_values.items() if q == max_q_value]\n",
    "            action = np.random.choice(actions_with_max_q_value)\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_q(self, state, action, next_state, reward, alpha, gamma):\n",
    "        # Update Q-value for the given state-action pair\n",
    "        td_target = reward + gamma * np.max(self.q_table[next_state])\n",
    "        td_error = td_target - self.q_table[state][self.actions.index(action)]\n",
    "        self.q_table[state][self.actions.index(action)] += alpha * td_error\n",
    "\n",
    "    def run_episode(self, env):\n",
    "        # Reset the environment and initialize the state\n",
    "        obs = env.reset()\n",
    "        state = self.get_state(obs)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action and take a step in the environment\n",
    "            action = self.choose_action(state, self.epsilon)\n",
    "            obs, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the state and Q-table\n",
    "            next_state = self.get_state(obs)\n",
    "            self.update_q(state, action, next_state, reward, self.alpha, self.discount_factor)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def train(self, env):\n",
    "        # Initialize the Q-table\n",
    "        self.q_table = {}\n",
    "\n",
    "        for i in range(self.num_episodes):\n",
    "            obs = env.reset()\n",
    "            state = tuple(self.get_state(obs).items())\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            epsilon = self.get_epsilon(i)\n",
    "\n",
    "            while not done:\n",
    "                # Choose an action and take a step in the environment\n",
    "                action = self.choose_action(state, epsilon)\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q-table\n",
    "                next_state = tuple(self.get_state(next_obs).items())\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "            # Update the learning rate and epsilon\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            self.eps = self.eps * self.eps_decay\n",
    "\n",
    "        return self.q_table\n",
    "    \n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((episode + 1) * self.epsilon_decay)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2360e0c6-34ef-4f4b-aa8f-6f371d3413b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "# !pip install unrar\n",
    "# !unrar x Roms.rar\n",
    "# !mkdir rars\n",
    "# !mv HC\\ ROMS.zip   rars\n",
    "# !mv ROMS.zip  rars\n",
    "# !python -m atari_py.import_roms rars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4ac949-210e-4e5c-9226-b3e768bfa16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd126e2-9cce-4e46-9d11-fdba10675092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m atari_py.import_roms /path/to/roms/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bc8cd9-6eaa-42d6-8e90-e09e61b540ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "VersionNotFound",
     "evalue": "Environment version `v5` for environment `MsPacman` doesn't exist. It provides versioned environments: [ `v0`, `v4` ].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVersionNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the environment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !python -m atari_py.import_roms Roms\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"MsPacman-v4\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"ALE/MsPacman-v5\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMsPacman-v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create the agent\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pacman_rl \u001b[38;5;241m=\u001b[39m PacManRL(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:248\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    245\u001b[0m     version_list_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec_\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m spec_ \u001b[38;5;129;01min\u001b[39;00m env_specs)\n\u001b[1;32m    246\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It provides versioned environments: [ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion_list_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mVersionNotFound(message)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latest_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m version \u001b[38;5;241m<\u001b[39m latest_spec\u001b[38;5;241m.\u001b[39mversion:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDeprecatedEnv(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment version v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_env_id(ns, name, \u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n",
      "\u001b[0;31mVersionNotFound\u001b[0m: Environment version `v5` for environment `MsPacman` doesn't exist. It provides versioned environments: [ `v0`, `v4` ]."
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "# !python -m atari_py.import_roms Roms\n",
    "# env = gym.make(\"MsPacman-v4\")\n",
    "# rom_path = \"/path/to/roms/ms_pacman.bin\" \n",
    "# env = gym.make(\"ALE/MsPacman-v5\", rom_path=rom_path)\n",
    "\n",
    "# env = gym.make(\"ALE/MsPacman-v5\")\n",
    "env = gym.make(\"MsPacman-v5\")\n",
    "\n",
    "# Create the agent\n",
    "pacman_rl = PacManRL(num_episodes=1000, epsilon_min=0.01)\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "q_table = pacman_rl.train(env)\n",
    "\n",
    "# Test the agent\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    state = pacman_rl.get_state(obs)\n",
    "    action = np.argmax(q_table[state])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec5905-0aa6-4b57-8c14-5625ea1fffe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543c76c-9d32-444a-81ef-a0a693cc03e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2624f-1726-423f-8533-32ee4647174b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
