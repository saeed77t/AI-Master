{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c37d578-998f-494c-a886-bd776c0e2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import gym\n",
    "\n",
    "\n",
    "class PacManRL:\n",
    "    def __init__(self, num_episodes=1000, gamma=0.99, alpha=0.5, epsilon=1.0, epsilon_decay=0.01, epsilon_min=0.01):\n",
    "        self.q_table = {}\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # Define the action space and initialize the Q-table\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.actions)))\n",
    "\n",
    "        # Define features for state representation\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "        \n",
    "        \n",
    "    def get_state(self, game_state):\n",
    "        # Extract relevant information from the game state to represent it as a state\n",
    "        if isinstance(game_state, dict):\n",
    "            pacman_pos = game_state.get('pacman_pos', ())\n",
    "            food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "            ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "        else:\n",
    "            pacman_pos = ()\n",
    "            food_pos = ()\n",
    "            ghost_pos = ()\n",
    "\n",
    "        # Compute the remaining food left\n",
    "        if self.food_left is None:\n",
    "            self.food_left = len(food_pos)\n",
    "        else:\n",
    "            self.food_left -= self.food_eaten\n",
    "\n",
    "        # Compute the distance to the nearest food and the nearest ghost\n",
    "        dist_to_food = np.inf\n",
    "        for pos in food_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_food:\n",
    "                dist_to_food = dist\n",
    "\n",
    "        dist_to_ghost = np.inf\n",
    "        for pos in ghost_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_ghost:\n",
    "                dist_to_ghost = dist\n",
    "\n",
    "        # Compute the new features\n",
    "        self.food_eaten = self.food_left - len(food_pos)\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'food_distance': dist_to_food,\n",
    "            'ghost_distance': dist_to_ghost\n",
    "        }\n",
    "\n",
    "        return {'pacman_pos': pacman_pos, 'food_pos': food_pos, 'ghost_pos': ghost_pos}\n",
    "\n",
    "\n",
    "\n",
    "#     def get_state(self, game_state):\n",
    "#         # Extract relevant information from the game state to represent it as a state\n",
    "#         pacman_pos = tuple(game_state['pacman_pos'])\n",
    "#         food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "#         ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "\n",
    "#         # Compute the remaining food left\n",
    "#         if self.food_left is None:\n",
    "#             self.food_left = len(food_pos)\n",
    "#         else:\n",
    "#             self.food_left -= self.food_eaten\n",
    "\n",
    "#         # Compute the distance to the nearest food and the nearest ghost\n",
    "#         dist_to_food = np.inf\n",
    "#         for pos in food_pos:\n",
    "#             dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "#             if dist < dist_to_food:\n",
    "#                 dist_to_food = dist\n",
    "\n",
    "#         dist_to_ghost = np.inf\n",
    "#         for pos in ghost_pos:\n",
    "#             dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "#             if dist < dist_to_ghost:\n",
    "#                 dist_to_ghost = dist\n",
    "\n",
    "#         # Compute the new features\n",
    "#         self.food_eaten = self.food_left - len(food_pos)\n",
    "#         features = {\n",
    "#             'bias': 1.0,\n",
    "#             'food_distance': dist_to_food,\n",
    "#             'ghost_distance': dist_to_ghost\n",
    "#         }\n",
    "\n",
    "#         pacman_pos = tuple(game_state.get('pacman_pos', ()))\n",
    "#         food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "#         ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "#         return {'pacman_pos': pacman_pos, 'food_pos': food_pos, 'ghost_pos': ghost_pos}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = self.q_table.get(state, {a: 0 for a in self.actions})\n",
    "            max_q_value = max(q_values.values())\n",
    "            actions_with_max_q_value = [a for a, q in q_values.items() if q == max_q_value]\n",
    "            action = np.random.choice(actions_with_max_q_value)\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_q(self, state, action, next_state, reward, alpha, gamma):\n",
    "        # Update Q-value for the given state-action pair\n",
    "        td_target = reward + gamma * np.max(self.q_table[next_state])\n",
    "        td_error = td_target - self.q_table[state][self.actions.index(action)]\n",
    "        self.q_table[state][self.actions.index(action)] += alpha * td_error\n",
    "\n",
    "    def run_episode(self, env):\n",
    "        # Reset the environment and initialize the state\n",
    "        obs = env.reset()\n",
    "        state = self.get_state(obs)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action and take a step in the environment\n",
    "            action = self.choose_action(state, self.epsilon)\n",
    "            obs, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the state and Q-table\n",
    "            next_state = self.get_state(obs)\n",
    "            self.update_q(state, action, next_state, reward, self.alpha, self.discount_factor)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def train(self, env):\n",
    "        # Initialize the Q-table\n",
    "        self.q_table = {}\n",
    "\n",
    "        for i in range(self.num_episodes):\n",
    "            obs = env.reset()\n",
    "            state = tuple(self.get_state(obs).items())\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            epsilon = self.get_epsilon(i)\n",
    "\n",
    "            while not done:\n",
    "                # Choose an action and take a step in the environment\n",
    "                action = self.choose_action(state, epsilon)\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q-table\n",
    "                next_state = tuple(self.get_state(next_obs).items())\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "            # Update the learning rate and epsilon\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            self.eps = self.eps * self.eps_decay\n",
    "\n",
    "        return self.q_table\n",
    "    \n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((episode + 1) * self.epsilon_decay)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2360e0c6-34ef-4f4b-aa8f-6f371d3413b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "# !pip install unrar\n",
    "# !unrar x Roms.rar\n",
    "# !mkdir rars\n",
    "# !mv HC\\ ROMS.zip   rars\n",
    "# !mv ROMS.zip  rars\n",
    "# !python -m atari_py.import_roms rars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4ac949-210e-4e5c-9226-b3e768bfa16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd126e2-9cce-4e46-9d11-fdba10675092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m atari_py.import_roms /path/to/roms/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bc8cd9-6eaa-42d6-8e90-e09e61b540ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the environment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !python -m atari_py.import_roms Roms\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"MsPacman-v4\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"ALE/MsPacman-v5\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMsPacman-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create the agent\u001b[39;00m\n\u001b[1;32m     12\u001b[0m pacman_rl \u001b[38;5;241m=\u001b[39m PacManRL(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:652\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    647\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed render_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m although \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt implement human-rendering natively. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         )\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[39;00m\n\u001b[1;32m    655\u001b[0m spec_ \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(spec_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ale_py/env/gym.py:155\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, max_num_frames_per_episode, render_mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetBool(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Seed + Load\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetLegalActionSet()\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m full_action_space\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetMinimalActionSet()\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ale_py/env/gym.py:205\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# ALE only takes signed integers for `setInt`, it'll get converted back\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# to unsigned in StellaEnvironment.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_game\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mre Unable to find the game \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Note: Gym no longer distributes ROMs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mloadROM(\u001b[38;5;28mgetattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ale_py/roms/__init__.py:118\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m roms:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ROM named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Supported ROMs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(roms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_rom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to resolve ROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` from plugins \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m plugin: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(plugin)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m, _ROM_PLUGIN_REGISTRY))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ale_py/roms/__init__.py:48\u001b[0m, in \u001b[0;36m_resolve_rom\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Resolve ROM from package\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     rom_path \u001b[38;5;241m=\u001b[39m \u001b[43mpackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrom_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ale_py/roms/plugins.py:39\u001b[0m, in \u001b[0;36mPackage.resolve\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[pathlib\u001b[38;5;241m.\u001b[39mPath]:\n\u001b[0;32m---> 39\u001b[0m     rom \u001b[38;5;241m=\u001b[39m \u001b[43mresources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rom\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/resources.py:147\u001b[0m, in \u001b[0;36mfiles\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfiles\u001b[39m(package: Package) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources_abc\u001b[38;5;241m.\u001b[39mTraversable:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Get a Traversable resource from a package\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_common\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_common.py:14\u001b[0m, in \u001b[0;36mfrom_package\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_package\u001b[39m(package):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Return a Traversable object for the given package.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfallback_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__spec__\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_common.py:18\u001b[0m, in \u001b[0;36mfallback_resources\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfallback_resources\u001b[39m(spec):\n\u001b[0;32m---> 18\u001b[0m     package_directory \u001b[38;5;241m=\u001b[39m \u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         archive_path \u001b[38;5;241m=\u001b[39m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39marchive\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pathlib.py:1082\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pathlib.py:707\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args, init)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 707\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pathlib.py:691\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    689\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "# !python -m atari_py.import_roms Roms\n",
    "# env = gym.make(\"MsPacman-v4\")\n",
    "# rom_path = \"/path/to/roms/ms_pacman.bin\" \n",
    "# env = gym.make(\"ALE/MsPacman-v5\", rom_path=rom_path)\n",
    "\n",
    "\n",
    "# env = gym.make(\"ALE/MsPacman-v5\")\n",
    "env = gym.make(\"MsPacman-v4\")\n",
    "\n",
    "# Create the agent\n",
    "pacman_rl = PacManRL(num_episodes=1000, epsilon_min=0.01)\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "q_table = pacman_rl.train(env)\n",
    "\n",
    "# Test the agent\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    state = pacman_rl.get_state(obs)\n",
    "    action = np.argmax(q_table[state])\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8e253-cefd-4d79-9975-3a24ad094f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atari_py\n",
    "\n",
    "games = atari_py.atari_roms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec5905-0aa6-4b57-8c14-5625ea1fffe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543c76c-9d32-444a-81ef-a0a693cc03e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2624f-1726-423f-8533-32ee4647174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "\n",
    "# env = gym.make(\"MsPacman-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca4831-ba9d-4d2c-9943-3b99328f9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Get the dictionary of registered environments\n",
    "env_dict = gym.envs.registration.registry\n",
    "\n",
    "# Extract the environment IDs that start with \"MsPacman\"\n",
    "pacman_envs = [env_id for env_id in env_dict.keys() if env_id.startswith('MsPacman')]\n",
    "\n",
    "# Print out the list of available MsPacman environments\n",
    "print(pacman_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406608c5-8b9d-44b7-8add-879abe522ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make('ALE/MsPacman-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ac8a5-f95c-43ae-a852-dd87e343dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import gym\n",
    "\n",
    "\n",
    "class PacManRL:\n",
    "    def __init__(self, num_episodes=1000, gamma=0.99, alpha=0.5, epsilon=1.0, epsilon_decay=0.01, epsilon_min=0.01):\n",
    "        self.q_table = {}\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # Define the action space and initialize the Q-table\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.actions)))\n",
    "\n",
    "        # Define features for state representation\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.food_left = None\n",
    "        self.food_eaten = None\n",
    "        \n",
    "        \n",
    "    def get_state(self, game_state):\n",
    "        # Extract relevant information from the game state to represent it as a state\n",
    "        if isinstance(game_state, dict):\n",
    "            pacman_pos = game_state.get('pacman_pos', ())\n",
    "            food_pos = tuple(map(tuple, np.argwhere(game_state['food'])))\n",
    "            ghost_pos = tuple(map(tuple, game_state['ghost_pos']))\n",
    "        else:\n",
    "            pacman_pos = ()\n",
    "            food_pos = ()\n",
    "            ghost_pos = ()\n",
    "\n",
    "        # Compute the remaining food left\n",
    "        if self.food_left is None:\n",
    "            self.food_left = len(food_pos)\n",
    "        else:\n",
    "            self.food_left -= self.food_eaten\n",
    "\n",
    "        # Compute the distance to the nearest food and the nearest ghost\n",
    "        dist_to_food = np.inf\n",
    "        for pos in food_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_food:\n",
    "                dist_to_food = dist\n",
    "\n",
    "        dist_to_ghost = np.inf\n",
    "        for pos in ghost_pos:\n",
    "            dist = np.linalg.norm(np.array(pacman_pos) - np.array(pos))\n",
    "            if dist < dist_to_ghost:\n",
    "                dist_to_ghost = dist\n",
    "\n",
    "        # Compute the new features\n",
    "        self.food_eaten = self.food_left - len(food_pos)\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'food_distance': dist_to_food,\n",
    "            'ghost_distance': dist_to_ghost\n",
    "        }\n",
    "\n",
    "        return {'pacman_pos': pacman_pos, 'food_pos': food_pos, 'ghost_pos': ghost_pos}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = self.q_table.get(state, {a: 0 for a in self.actions})\n",
    "            max_q_value = max(q_values.values())\n",
    "            actions_with_max_q_value = [a for a, q in q_values.items() if q == max_q_value]\n",
    "            action = np.random.choice(actions_with_max_q_value)\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_q(self, state, action, next_state, reward, alpha, gamma):\n",
    "        # Update Q-value for the given state-action pair\n",
    "        td_target = reward + gamma * np.max(self.q_table[next_state])\n",
    "        td_error = td_target - self.q_table[state][self.actions.index(action)]\n",
    "        self.q_table[state][self.actions.index(action)] += alpha * td_error\n",
    "\n",
    "    def run_episode(self, env):\n",
    "        # Reset the environment and initialize the state\n",
    "        obs = env.reset()\n",
    "        state = self.get_state(obs)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action and take a step in the environment\n",
    "            action = self.choose_action(state, self.epsilon)\n",
    "            obs, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the state and Q-table\n",
    "            next_state = self.get_state(obs)\n",
    "            self.update_q(state, action, next_state, reward, self.alpha, self.discount_factor)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def train(self, env):\n",
    "        # Initialize the Q-table\n",
    "        self.q_table = {}\n",
    "\n",
    "        for i in range(self.num_episodes):\n",
    "            obs = env.reset()\n",
    "            state = tuple(self.get_state(obs).items())\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            epsilon = self.get_epsilon(i)\n",
    "\n",
    "            while not done:\n",
    "                # Choose an action and take a step in the environment\n",
    "                action = self.choose_action(state, epsilon)\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q-table\n",
    "                next_state = tuple(self.get_state(next_obs).items())\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "            # Update the learning rate and epsilon\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            self.eps = self.eps * self.eps_decay\n",
    "\n",
    "        return self.q_table\n",
    "    \n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((episode + 1) * self.epsilon_decay)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
