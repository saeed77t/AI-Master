{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00aa248a-9a7a-4db7-9787-cbcaf18a99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.tree import HellingerDecisionTree  #not working for me !\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcf6f98d-bc0b-46bd-8516-f0869b29c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -->> I dont know if it is correct or not from part A ! \n",
    "class HellingerDecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self._predict_tree(x, self.tree))\n",
    "        return np.array(y_pred)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if depth == self.max_depth or n_samples == 1:\n",
    "            return np.round(y.mean())\n",
    "        \n",
    "        feature_idxs = np.random.choice(n_features, size=int(np.sqrt(n_features)), replace=False)\n",
    "        \n",
    "        best_feature, best_threshold = self._best_criteria(X, y, feature_idxs)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        \n",
    "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        \n",
    "        return (best_feature, best_threshold, left, right)\n",
    "    \n",
    "    def _best_criteria(self, X, y, feature_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        for feature_idx in feature_idxs:\n",
    "            X_feature = X[:, feature_idx]\n",
    "            thresholds = np.unique(X_feature)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_idx\n",
    "                    split_threshold = threshold\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "        \n",
    "    def _information_gain(self, y, X_feature, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left_idxs, right_idxs = self._split(X_feature, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        entropy_l, entropy_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * entropy_l + (n_r / n) * entropy_r\n",
    "        return parent_entropy - child_entropy\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = sum(probabilities * -np.log2(probabilities))\n",
    "        return entropy\n",
    "    \n",
    "    def _split(self, X_feature, threshold):\n",
    "        left_idxs = np.argwhere(X_feature <= threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_feature > threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _predict_tree(self, x, tree):\n",
    "        if isinstance(tree, np.float64):\n",
    "            return tree\n",
    "        feature, threshold, left, right = tree\n",
    "        if x[feature] <= threshold:\n",
    "            return self._predict_tree(x, left)\n",
    "        else:\n",
    "            return self._predict_tree(x, right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56e6bd95-c40c-431e-a394-24fae2314704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other implemention of HDDT : not sure if its true ! \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A class to represent a node in the decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class HellingerDistanceDecisionTree:\n",
    "    \"\"\"\n",
    "    A decision tree based on Hellinger distance.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree on the given training data.\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels of the given data using the trained tree.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            node = self.root\n",
    "            while node.left:\n",
    "                if X[i, node.feature_idx] < node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(node.value)\n",
    "        return predictions\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Build the decision tree recursively.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Check for termination conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) \\\n",
    "                or n_samples < self.min_samples_split \\\n",
    "                or len(np.unique(y)) == 1:\n",
    "            leaf_value = self._leaf_value(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature_idx, best_threshold = self._best_split(X, y, n_samples, n_features, n_classes)\n",
    "\n",
    "        # Check for termination conditions after finding the best split\n",
    "        if best_feature_idx is None or best_threshold is None:\n",
    "            leaf_value = self._leaf_value(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left_idxs = X[:, best_feature_idx] < best_threshold\n",
    "        right_idxs = ~left_idxs\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self._build_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node(best_feature_idx, best_threshold, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, n_samples, n_features, n_classes):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the data.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Calculate the Hellinger distance for the parent node\n",
    "        parent_value = self._leaf_value(y)\n",
    "        parent_score = self._hddt_criterion(y, parent_value, n_classes)\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_idxs = X[:, feature_idx] < threshold\n",
    "                n_left = np.sum(left_idxs)\n",
    "                n_right = n_samples - n_left\n",
    "\n",
    "                if n_left >= self.min_samples_leaf and n_right >= self.min_samples_leaf:\n",
    "                    left_value = self._leaf_value(y[left_idxs])\n",
    "                    right_value = self._leaf_value(y[~left_idxs])\n",
    "                    gain = self._information_gain(parent_score, left_value, right_value)\n",
    "\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_feature_idx = feature_idx\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "    \n",
    "    def _hddt_criterion(self, y, value, n_classes):\n",
    "        \"\"\"\n",
    "        Compute the Hellinger distance-based decision tree criterion.\n",
    "        \"\"\"\n",
    "        # Compute the histogram of classes in the data\n",
    "        hist = np.histogram(y, bins=n_classes, range=(0, n_classes))[0]\n",
    "\n",
    "        # Compute the probabilities of each class in the data\n",
    "        p = hist / len(y)\n",
    "\n",
    "        # Compute the histogram of classes in the left and right subsets\n",
    "        left_hist = np.histogram(value[0], bins=n_classes, range=(0, n_classes))[0]\n",
    "        right_hist = np.histogram(value[1], bins=n_classes, range=(0, n_classes))[0]\n",
    "\n",
    "        # Compute the probabilities of each class in the left and right subsets\n",
    "        left_p = left_hist / len(value[0])\n",
    "        right_p = right_hist / len(value[1])\n",
    "\n",
    "        # Compute the Hellinger distance between the parent node and its two children\n",
    "        dist_left = np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(left_p))**2))\n",
    "        dist_right = np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(right_p))**2))\n",
    "\n",
    "        # Compute the gain in Hellinger distance\n",
    "        gain = np.abs(dist_left - dist_right)\n",
    "\n",
    "        return gain\n",
    "\n",
    "    def _leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Compute the value of a leaf node (i.e., the class that appears most frequently in the data).\n",
    "        \"\"\"\n",
    "        hist = np.histogram(y, bins=len(np.unique(y)))[0]\n",
    "        return np.argmax(hist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31790e86-3d44-4a82-a4b3-0791999410ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part i have to implement !\n",
    "class BaggingClassifier:\n",
    "    \n",
    "    def __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []\n",
    "        self.estimators_samples_ = []\n",
    "        self.estimators_features_ = []\n",
    "        self.classes_ = None\n",
    "        self.n_features_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        for i in range(self.n_estimators):\n",
    "            indices = rng.choice(len(X), int(self.max_samples * len(X)), replace=True)\n",
    "            features = rng.choice(self.n_features_, int(self.max_features * self.n_features_), replace=False)\n",
    "            estimator = self.base_estimator.fit(X[indices][:, features], y[indices])\n",
    "            self.estimators_.append(estimator)\n",
    "            self.estimators_samples_.append(indices)\n",
    "            self.estimators_features_.append(features)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators), dtype=int)\n",
    "        for i, estimator in enumerate(self.estimators_):\n",
    "            predictions[:, i] = estimator.predict(X[:, self.estimators_features_[i]])\n",
    "        return np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probas = np.zeros((X.shape[0], self.n_estimators, len(self.classes_)))\n",
    "        for i, estimator in enumerate(self.estimators_):\n",
    "            probas[:, i, :] = estimator.predict_proba(X[:, self.estimators_features_[i]])\n",
    "        return np.mean(probas, axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09bab451-1ec1-4600-bf25-33c126e50742",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'criterion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define the base learners\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=5)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# hddt_clf = HellingerDecisionTree(criterion='entropy', max_depth=5)   -->> ToDo : define HellingerDecisionTree!\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# dt_clf = DecisionTreeClassifier()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m hddt_clf \u001b[38;5;241m=\u001b[39m \u001b[43mHellingerDecisionTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define the Bagging classifier with the base learners\u001b[39;00m\n\u001b[1;32m     17\u001b[0m bagging_hddt_clf \u001b[38;5;241m=\u001b[39m BaggingClassifier(base_estimator\u001b[38;5;241m=\u001b[39mhddt_clf, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'criterion'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, weights=[0.1, 0.9], random_state=42)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the base learners\n",
    "# dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "# hddt_clf = HellingerDecisionTree(criterion='entropy', max_depth=5)   -->> ToDo : define HellingerDecisionTree!\n",
    "# dt_clf = DecisionTreeClassifier()\n",
    "hddt_clf = HellingerDecisionTree(criterion='entropy', max_depth=5)\n",
    "\n",
    "\n",
    "# Define the Bagging classifier with the base learners\n",
    "bagging_hddt_clf = BaggingClassifier(base_estimator=hddt_clf, n_estimators=100, max_samples=0.5, max_features=0.5)\n",
    "# bagging_hddt_clf = BaggingClassifier(base_estimator=hddt_clf, n_estimators=100, max_samples=0.5, max_features=0.5)\n",
    "\n",
    "bagging_dt_clf = BaggingClassifier(base_estimator=dt_clf, n_estimators=100, max_samples=0.5, max_features=0.5)\n",
    "\n",
    "# Train the Bagging classifiers on the training set\n",
    "bagging_hddt_clf.fit(X_train, y_train)\n",
    "bagging_dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_hddt = bagging_hddt_clf.predict(X_test)\n",
    "y_pred_dt = bagging_dt_clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate performance metrics\n",
    "precision = precision_score(y_test, y_pred_hddt)\n",
    "recall = recall_score(y_test, y_pred_hddt)\n",
    "f1 = f1_score(y_test, y_pred_hddt)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_hddt)\n",
    "confusion = confusion_matrix(y_test, y_pred_hddt)\n",
    "classification = classification_report(y_test, y_pred_hddt)\n",
    "accuracy = accuracy_score(y_test, y_pred_hddt)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_hddt)\n",
    "g_mean = np.sqrt(recall * (1 - precision))\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Performance metrics for Bagging with Hellinger decision trees:')\n",
    "print('Precision: {:.4f}'.format(precision))\n",
    "print('Recall: {:.4f}'.format(recall))\n",
    "print('F1-score: {:.4f}'.format(f1))\n",
    "print('ROC-AUC score: {:.4f}'.format(roc_auc))\n",
    "print('Confusion matrix:\\n', confusion)\n",
    "print('Classification report:\\n', classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e38439-eac0-4af7-8d27-e09260739609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c683b35-1aaa-44e9-9821-42a64dc7bfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bff7e9-0e3a-4ef7-bc45-7e280f9de3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fda2e2-cf3a-4147-a521-cc0d444b5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756d5e1-6a18-4f1a-8cea-345527759104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
