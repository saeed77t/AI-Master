{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2924c0e5-6e42-4385-8f59-a81b86f30d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# from sklearn.utils import check_X_y, check_array\n",
    "# from sklearn.utils.validation import check_is_fitted\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cbf28b9-b989-41fb-b3c1-0fc8df3bdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class AdaBoostClassifier:\n",
    "#     def __init__(self, n_estimators=50, learning_rate=1):\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.estimators = []\n",
    "#         self.estimator_weights = np.zeros(n_estimators)\n",
    "#         self.estimator_errors = np.ones(n_estimators)\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples = X.shape[0]\n",
    "#         sample_weights = np.full(n_samples, 1/n_samples)\n",
    "\n",
    "#         for i in range(self.n_estimators):\n",
    "#             estimator = DecisionTreeClassifier(max_depth=1)\n",
    "#             estimator.fit(X, y, sample_weight=sample_weights)\n",
    "#             y_pred = estimator.predict(X)\n",
    "\n",
    "#             incorrect = y_pred != y\n",
    "#             estimator_error = np.mean(np.average(incorrect, weights=sample_weights, axis=0))\n",
    "#             estimator_weight = self.learning_rate * np.log((1 - estimator_error) / estimator_error)\n",
    "\n",
    "#             sample_weights *= np.exp(estimator_weight * incorrect * ((sample_weights > 0) | (estimator_weight < 0)))\n",
    "#             sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "#             self.estimators.append(estimator)\n",
    "#             self.estimator_weights[i] = estimator_weight\n",
    "#             self.estimator_errors[i] = estimator_error\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         y_pred = np.zeros(X.shape[0])\n",
    "#         for i, estimator in enumerate(self.estimators):\n",
    "#             y_pred += self.estimator_weights[i] * estimator.predict(X)\n",
    "#         return np.sign(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f465a7-0d5e-4912-bcbb-59426f9749c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaboost_undersample(X, y, T=10, s=11):\n",
    "#     minority_class = X[y == 1]\n",
    "#     majority_class = X[y == -1]\n",
    "#     n_minority = minority_class.shape[0]\n",
    "\n",
    "#     estimators = []\n",
    "#     for i in range(T):\n",
    "#         sample_indices = np.random.choice(majority_class.shape[0], n_minority, replace=False)\n",
    "#         sample_majority = majority_class[sample_indices]\n",
    "#         X_sampled = np.vstack((minority_class, sample_majority))\n",
    "#         y_sampled = np.concatenate(([1] * n_minority, [-1] * n_minority))\n",
    "\n",
    "#         clf = AdaBoostClassifier(n_estimators=s)\n",
    "#         clf.fit(X_sampled, y_sampled)\n",
    "#         estimators.extend(clf.estimators)\n",
    "\n",
    "#     return estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27764950-f455-4649-a427-8c1fe473aff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9c0b8-67f9-4e1f-bf7b-0e2b852e5e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb93025-1907-45ee-8318-e2a8c2178988",
   "metadata": {},
   "source": [
    "Some Other implemention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5209129d-74ca-4f5c-973b-0b895d49f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# class AdaBoostClassifier:\n",
    "#     def __init__(self, n_estimators=50, learning_rate=1.0, random_state=None):\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.estimators_ = []\n",
    "#         self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "#         self.errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "#         self.sample_weights_ = None\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def fit(self, X, y, sample_weight=None):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self.sample_weights_ = np.zeros(n_samples, dtype=np.float64) + 1 / n_samples\n",
    "\n",
    "#         for i in range(self.n_estimators):\n",
    "#             estimator = DecisionTreeClassifier(max_depth=1, random_state=self.random_state)\n",
    "#             indices = np.random.choice(n_samples, n_samples, p=self.sample_weights_, replace=True, random_state=self.random_state)\n",
    "#             estimator.fit(X[indices], y[indices], sample_weight=self.sample_weights_[indices])\n",
    "#             self.estimators_.append(estimator)\n",
    "\n",
    "#             y_pred = estimator.predict(X)\n",
    "#             incorrect = y_pred != y\n",
    "#             self.errors_[i] = np.sum(self.sample_weights_[incorrect])\n",
    "\n",
    "#             alpha = self.learning_rate * np.log((1 - self.errors_[i]) / self.errors_[i])\n",
    "#             self.estimator_weights_[i] = alpha\n",
    "\n",
    "#             self.sample_weights_ *= np.exp(alpha * incorrect * (self.sample_weights_ > 0))\n",
    "#             self.sample_weights_ /= np.sum(self.sample_weights_)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         n_samples = X.shape[0]\n",
    "#         pred = np.zeros((n_samples, self.n_estimators))\n",
    "\n",
    "#         for i in range(self.n_estimators):\n",
    "#             pred[:, i] = self.estimators_[i].predict(X)\n",
    "\n",
    "#         pred = np.dot(pred, self.estimator_weights_)\n",
    "#         classes = np.unique(self.estimators_[0].classes_)\n",
    "        \n",
    "#         if len(classes) == 2:\n",
    "#             pred = np.sign(pred)\n",
    "#         else:\n",
    "#             pred = classes[np.argmax(pred, axis=1)]\n",
    "\n",
    "#         return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14918e3-1d94-4f9f-8d38-89b4e93661cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdaBoostUnderSample(AdaBoostClassifier):\n",
    "    \n",
    "#     def __init__(self, n_estimators=50, learning_rate=1.0,  random_state=42, balance_ratio=0.5):\n",
    "#         super().__init__( n_estimators=n_estimators, learning_rate=learning_rate,  random_state=42)\n",
    "#         self.balance_ratio = balance_ratio\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         # separate majority and minority classes\n",
    "#         X_majority = X[y == 0]\n",
    "#         X_minority = X[y == 1]\n",
    "#         y_minority = y[y == 1]\n",
    "        \n",
    "#         # determine the number of samples to keep in the minority class\n",
    "#         n_minority = int(len(y_minority) * self.balance_ratio)\n",
    "        \n",
    "#         # balance the minority class by randomly downsampling\n",
    "#         X_minority_balanced, y_minority_balanced = resample(X_minority, y_minority, replace=False, \n",
    "#                                                             n_samples=n_minority, random_state=self.random_state)\n",
    "        \n",
    "#         # balance the majority class by randomly upsampling\n",
    "#         n_majority = n_minority\n",
    "#         X_majority_balanced, y_majority_balanced = resample(X_majority, y[y == 0], replace=True, \n",
    "#                                                             n_samples=n_majority, random_state=self.random_state)\n",
    "        \n",
    "#         # concatenate the balanced minority and majority samples\n",
    "#         X_balanced = np.concatenate((X_minority_balanced, X_majority_balanced), axis=0)\n",
    "#         y_balanced = np.concatenate((y_minority_balanced, y_majority_balanced), axis=0)\n",
    "        \n",
    "#         # fit base estimator on balanced subsample\n",
    "#         super().fit(X_balanced, y_balanced)\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         y_pred = np.zeros(X.shape[0])\n",
    "#         for t in range(self.n_estimators):\n",
    "#             y_pred += self.estimator_weights_[t] * self.estimators_[t].predict(X)\n",
    "#         return np.sign(y_pred)\n",
    "    \n",
    "#     def predict_proba(self, X):\n",
    "#         proba = np.zeros((X.shape[0], 2))\n",
    "#         for t in range(self.n_estimators):\n",
    "#             proba += self.estimator_weights_[t] * self.estimators_[t].predict_proba(X)\n",
    "#         proba /= np.sum(self.estimator_weights_)\n",
    "#         return proba\n",
    "    \n",
    "#     def score(self, X, y):\n",
    "#         return accuracy_score(y, self.predict(X))\n",
    "\n",
    "# # generate synthetic dataset\n",
    "# X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# # split dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print('shape of X_train : ', X_train.shape)\n",
    "# # create AdaBoost classifier with under-sampling\n",
    "# adaboost = AdaBoostUnderSample()\n",
    "\n",
    "# # fit the classifier to the training data\n",
    "# adaboost.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions on the testing data\n",
    "# y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# # evaluate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c072b1a9-44aa-4375-bd0c-61601c2adaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class AdaBoostUnderSample:\n",
    "    \n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0, random_state=42, balance_ratio=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.balance_ratio = balance_ratio\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators)\n",
    "        self.errors_ = np.ones(self.n_estimators)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # separate majority and minority classes\n",
    "        X_majority = X[y == 0]\n",
    "        X_minority = X[y == 1]\n",
    "        y_minority = y[y == 1]\n",
    "        \n",
    "        # determine the number of samples to keep in the minority class\n",
    "        n_minority = int(len(y_minority) * self.balance_ratio)\n",
    "        \n",
    "        # balance the minority class by randomly downsampling\n",
    "        X_minority_balanced, y_minority_balanced = resample(X_minority, y_minority, replace=False, \n",
    "                                                            n_samples=n_minority, random_state=self.random_state)\n",
    "        \n",
    "        # balance the majority class by randomly upsampling\n",
    "        n_majority = n_minority\n",
    "        X_majority_balanced, y_majority_balanced = resample(X_majority, y[y == 0], replace=True, \n",
    "                                                            n_samples=n_majority, random_state=self.random_state)\n",
    "        \n",
    "        # concatenate the balanced minority and majority samples\n",
    "        X_balanced = np.concatenate((X_minority_balanced, X_majority_balanced), axis=0)\n",
    "        y_balanced = np.concatenate((y_minority_balanced, y_majority_balanced), axis=0)\n",
    "        \n",
    "        # initialize sample weights\n",
    "        sample_weights = np.ones(len(y_balanced)) / len(y_balanced)\n",
    "        \n",
    "        for t in range(self.n_estimators):\n",
    "            # train base estimator on weighted samples\n",
    "            base_estimator = DecisionTreeClassifier(max_depth=1, random_state=self.random_state)\n",
    "            base_estimator.fit(X_balanced, y_balanced, sample_weight=sample_weights)\n",
    "            \n",
    "            # compute error and estimator weight\n",
    "            y_pred = base_estimator.predict(X_balanced)\n",
    "            error = np.sum(sample_weights * (y_pred != y_balanced))\n",
    "            estimator_weight = self.learning_rate * (np.log(1 - error) - np.log(error)) + np.log(1)\n",
    "            \n",
    "            # update sample weights\n",
    "            sample_weights *= np.exp(-estimator_weight * y_balanced * y_pred)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "            # save estimator and weight\n",
    "            self.estimators_.append(base_estimator)\n",
    "            self.estimator_weights_[t] = estimator_weight\n",
    "            self.errors_[t] = error\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for t in range(self.n_estimators):\n",
    "            y_pred += self.estimator_weights_[t] * self.estimators_[t].predict(X)\n",
    "        return np.sign(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        proba = np.zeros((X.shape[0], 2))\n",
    "        for t in range(self.n_estimators):\n",
    "            proba += self.estimator_weights_[t] * self.estimators_[t].predict_proba(X)\n",
    "        proba /= np.sum(self.estimator_weights_)\n",
    "        return proba\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab18b906-2fcf-45cc-aba5-9424123bdb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train :  (8000, 20)\n",
      "Accuracy: 0.7195\n"
     ]
    }
   ],
   "source": [
    "# generate synthetic dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('shape of X_train : ', X_train.shape)\n",
    "# create AdaBoost classifier with under-sampling\n",
    "adaboost = AdaBoostUnderSample()\n",
    "\n",
    "# fit the classifier to the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77f81a-d8fb-46d9-8497-816f5e47815f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236aa06-7c77-4c5a-b5ce-09de7b55b84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286a0b5-c72f-48c8-a2df-acbab2dc2bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb093d87-30f2-433b-aa47-6dbe1b3ae05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d12e89-28c0-464e-8e4b-7e4e81c11422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a34ce-a7dd-4046-9676-056116f34310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9f3d2-b8a1-48b0-b8b5-5cb2995a86ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388a6bf-50ca-41f5-88c6-7444b3c5f78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a34c83-3e2d-4f9c-9bf0-00b2a4e24d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0434c54-b6cd-48cb-8d4b-a0898b9aa05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
