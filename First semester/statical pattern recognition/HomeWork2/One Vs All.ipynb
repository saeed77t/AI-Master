{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "406b2b97-1425-4b77-9556-2e96e2c4df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Data = pd.read_csv('seed.txt',header=None, sep=\"\\s+\", usecols=[0,1,2,3,4,5,6,7], names=['f1','f2','f3','f4','f5','f6','f7', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "750d59a9-6eca-4ad5-941e-5e071d8397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreprocessData:\n",
    "    def NomalizeData(Data):\n",
    "        lenght = len(Data)\n",
    "        normalizedData = []\n",
    "        for i in range (lenght):\n",
    "            normalizedData.append(float((Data[i] - min(Data) ) / ( max(Data) - min(Data)) ))\n",
    "            \n",
    "        return normalizedData\n",
    "    \n",
    "    def TestAndTrain(Data , PercentageOfTrainData):\n",
    "        PercentageOfTrainData = float(PercentageOfTrainData / 100)\n",
    "        Train_DataFrame = Data.sample(frac=PercentageOfTrainData)\n",
    "        Test_DataFrame =Data.drop(Train_DataFrame.index)\n",
    "        \n",
    "        return Train_DataFrame , Test_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "25cbf7ed-db3e-4254-b484-6b1e68af0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 8)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devide data to test and train \n",
    "Dataset = PreprocessData.TestAndTrain(Data , 80)\n",
    "\n",
    "TrainData = Dataset[0]\n",
    "TestData = Dataset[1]\n",
    "X = TrainData[['f1','f2','f3','f4','f5','f6','f7']].to_numpy()\n",
    "Xnonbias = X\n",
    "X= np.c_[np.ones((len(X) , 1)) , X]\n",
    "Y = TrainData['label'].to_numpy()\n",
    "\n",
    "xtest =  TestData[['f1','f2','f3','f4','f5','f6','f7']].to_numpy()\n",
    "ytest =  TestData['label'].to_numpy()\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ea41bdaa-9900-465a-9f42-df24842dbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData1 = TrainData[TrainData.label == 1]\n",
    "TrainData2 = TrainData[TrainData.label == 2]\n",
    "TrainData3 = TrainData[TrainData.label == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8eea6c-4652-40b2-a514-45b992900c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "dcff6610-8146-41a2-a686-95b1d5147aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+ np.exp(-1*z))\n",
    "\n",
    "def hypotesis( X , theta):\n",
    "    z = np.dot(X , theta)\n",
    "    yHat = sigmoid(z)\n",
    "    return yHat\n",
    "def error(X , theta , y ,classin):\n",
    "    cost = 0\n",
    "    for i , j in zip(X , y):\n",
    "        x_bar = np.array(np.insert(i, 0, 1))\n",
    "        y_hat = hypotesis(x_bar, theta)\n",
    "        classin = 1.0 if j == classin else 0.0\n",
    "        cost += classin * np.log(y_hat) + (1.0 - classin) * np.log(1 - y_hat)\n",
    "        \n",
    "    return cost   \n",
    "    \n",
    "def train( input_var, label, itrations , classin , alpharate ,theta):\n",
    "    classofInterst = classin\n",
    "    theta1 = theta\n",
    "    for a in range(itrations):\n",
    "        \n",
    "        \n",
    "        if a % 1000 == 0:\n",
    "            print(f'iteration: {a}')\n",
    "            print(f'cost: {error(input_var,  theta1, label , classofInterst)}')\n",
    "            print(theta1)\n",
    "            print('--------------------------------------------')\n",
    "        \n",
    "        \n",
    "        for i , xy in enumerate(zip(input_var , label)):\n",
    "            x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "            y_hat = hypotesis(x_bar,theta)\n",
    "            \n",
    "            y_binary = 1.0 if xy[1] == classin else 0.0 \n",
    "             \n",
    "            gradient = (y_binary - y_hat) * x_bar\n",
    "            theta += alpharate * gradient\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    return theta\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "aea724d5-28ea-42a8-81d6-950f0830c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "cost: -116.448726334071\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "--------------------------------------------\n",
      "iteration: 1000\n",
      "cost: -91.84211386681518\n",
      "[ 6.04266408 -2.53304061  3.4254251   1.111151    0.39220865  3.3362598\n",
      " -8.38495561 -9.6068238 ]\n",
      "--------------------------------------------\n",
      "iteration: 2000\n",
      "cost: -96.76701238618818\n",
      "[  7.61771859  -2.17773566   4.57077654   1.62696037   0.28228389\n",
      "   5.49294884  -8.1076525  -17.47654619]\n",
      "--------------------------------------------\n",
      "iteration: 3000\n",
      "cost: -100.80840009206554\n",
      "[  8.62022439  -1.88131152   5.59794713   2.00290378   0.48406676\n",
      "   7.12218087  -7.75822293 -24.04806116]\n",
      "--------------------------------------------\n",
      "iteration: 4000\n",
      "cost: -104.91519679847997\n",
      "[  9.31289273  -1.72439802   6.59071301   2.2875875    0.93710336\n",
      "   8.37612427  -7.53524044 -29.62760064]\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [257]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m params_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(Xnonbias\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m      7\u001b[0m a \u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXnonbias\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_0\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [256]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_var, label, itrations, classin, alpharate, theta)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , xy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(input_var , label)):\n\u001b[0;32m---> 33\u001b[0m     x_bar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m hypotesis(x_bar,theta)\n\u001b[1;32m     36\u001b[0m     y_binary \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xy[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m classin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4717\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   4712\u001b[0m values \u001b[38;5;241m=\u001b[39m array(values, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mndim, dtype\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4714\u001b[0m     \u001b[38;5;66;03m# broadcasting is very different here, since a[:,0,:] = ... behaves\u001b[39;00m\n\u001b[1;32m   4715\u001b[0m     \u001b[38;5;66;03m# very different from a[:,[0],:] = ...! This changes values so that\u001b[39;00m\n\u001b[1;32m   4716\u001b[0m     \u001b[38;5;66;03m# it works likes the second case. (here a[:,0:1,:])\u001b[39;00m\n\u001b[0;32m-> 4717\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoveaxis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4718\u001b[0m numnew \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m   4719\u001b[0m newshape[axis] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numnew\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmoveaxis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1454\u001b[0m, in \u001b[0;36mmoveaxis\u001b[0;34m(a, source, destination)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[1;32m   1452\u001b[0m     transpose \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose\n\u001b[0;32m-> 1454\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_axis_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m destination \u001b[38;5;241m=\u001b[39m normalize_axis_tuple(destination, a\u001b[38;5;241m.\u001b[39mndim, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(source) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(destination):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1380\u001b[0m, in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;66;03m# Optimization to speed-up the most common cases.\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(axis) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m         axis \u001b[38;5;241m=\u001b[39m [operator\u001b[38;5;241m.\u001b[39mindex(axis)]\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 1e-2\n",
    "params_0 = np.zeros(Xnonbias.shape[1] + 1)\n",
    "\n",
    "max_iter = 20000\n",
    "\n",
    "\n",
    "a =\\\n",
    "train(Xnonbias/ 8 , Y , 10000 , 1 , alpha , params_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cab2456b-3ea4-4882-8c24-f2b4493bef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"Class for training and using a model for logistic regression\"\"\"\n",
    "    \n",
    "    def set_values(self, initial_params, alpha=0.01, max_iter=5000, class_of_interest=0):\n",
    "        \"\"\"Set the values for initial params, step size, maximum iteration, and class of interest\"\"\"\n",
    "        self.params = initial_params\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.class_of_interest = class_of_interest\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Sigmoide function\"\"\"\n",
    "        \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, x_bar, params):\n",
    "        \"\"\"predict the probability of a class\"\"\"  \n",
    "\n",
    "        return self._sigmoid(np.dot(params, x_bar))\n",
    "        \n",
    "    \n",
    "    def _compute_cost(self, input_var, output_var, params):\n",
    "        \"\"\"Compute the log likelihood cost\"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for x, y in zip(input_var, output_var):\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, params)\n",
    "            \n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat)\n",
    "            \n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def train(self, input_var, label, print_iter = 5000):\n",
    "        \"\"\"Train the model using batch gradient ascent\"\"\"\n",
    "        \n",
    "        iteration = 1\n",
    "        while iteration < self.max_iter:\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f'iteration: {iteration}')\n",
    "                print(f'cost: {self._compute_cost(input_var, label, self.params)}')\n",
    "                print(self.params)\n",
    "                print('--------------------------------------------')\n",
    "            \n",
    "            for i, xy in enumerate(zip(input_var, label)):\n",
    "                x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "                y_hat = self.predict(x_bar, self.params)\n",
    "                \n",
    "                y_binary = 1.0 if xy[1] == self.class_of_interest else 0.0\n",
    "                gradient = (y_binary - y_hat) * x_bar\n",
    "                self.params += self.alpha * gradient\n",
    "            \n",
    "            iteration +=1\n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    def test(self, input_test, label_test):\n",
    "        \"\"\"Test the accuracy of the model using test data\"\"\"\n",
    "        self.total_classifications = 0\n",
    "        self.correct_classifications = 0\n",
    "        \n",
    "        for x,y in zip(input_test, label_test):\n",
    "            self.total_classifications += 1\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, self.params)\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            \n",
    "            if y_hat >= 0.5 and  y_binary == 1:\n",
    "                # correct classification of class_of_interest\n",
    "                self.correct_classifications += 1\n",
    "              \n",
    "            if y_hat < 0.5 and  y_binary != 1:\n",
    "                # correct classification of an other class\n",
    "                self.correct_classifications += 1\n",
    "                \n",
    "        self.accuracy = self.correct_classifications / self.total_classifications\n",
    "            \n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f95c1f90-64c9-43b4-a105-dcb258b82949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000\n",
      "cost: -64.70657761702192\n",
      "[ 6.04047207 -2.5331983   3.42406167  1.11052161  0.39244032  3.33375357\n",
      " -8.38473967 -9.59819297]\n",
      "--------------------------------------------\n",
      "iteration: 2000\n",
      "cost: -57.67574960368013\n",
      "[  7.61651477  -2.17810767   4.56972581   1.62652741   0.28223599\n",
      "   5.49108827  -8.10806311 -17.4693805 ]\n",
      "--------------------------------------------\n",
      "iteration: 3000\n",
      "cost: -52.902171572617156\n",
      "[  8.6193938   -1.88153192   5.59693779   2.00257822   0.48372539\n",
      "   7.12075871  -7.75850325 -24.04203077]\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m digits_regression_model_0 \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m      7\u001b[0m digits_regression_model_0\u001b[38;5;241m.\u001b[39mset_values(params_0, alpha, max_iter, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mdigits_regression_model_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXnonbias\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [254]\u001b[0m, in \u001b[0;36mLogisticRegression.train\u001b[0;34m(self, input_var, label, print_iter)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, xy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(input_var, label)):\n\u001b[1;32m     49\u001b[0m     x_bar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39minsert(xy[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     50\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x_bar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train a classifier for the ZERO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(Xnonbias.shape[1] + 1)\n",
    "\n",
    "max_iter = 80000\n",
    "digits_regression_model_0 = LogisticRegression()\n",
    "digits_regression_model_0.set_values(params_0, alpha, max_iter, 1)\n",
    "\n",
    "\n",
    "digits_regression_model_0.train(Xnonbias/ 8.0, Y, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a81047-419f-4aa5-9333-8e1f3ebc5eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
