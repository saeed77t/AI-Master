{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406b2b97-1425-4b77-9556-2e96e2c4df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Data = pd.read_csv('seed.txt',header=None, sep=\"\\s+\", usecols=[0,1,2,3,4,5,6,7], names=['f1','f2','f3','f4','f5','f6','f7', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750d59a9-6eca-4ad5-941e-5e071d8397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreprocessData:\n",
    "    def NomalizeData(Data):\n",
    "        lenght = len(Data)\n",
    "        normalizedData = []\n",
    "        for i in range (lenght):\n",
    "            normalizedData.append(float((Data[i] - min(Data) ) / ( max(Data) - min(Data)) ))\n",
    "            \n",
    "        return normalizedData\n",
    "    \n",
    "    def TestAndTrain(Data , PercentageOfTrainData):\n",
    "        PercentageOfTrainData = float(PercentageOfTrainData / 100)\n",
    "        Train_DataFrame = Data.sample(frac=PercentageOfTrainData)\n",
    "        Test_DataFrame =Data.drop(Train_DataFrame.index)\n",
    "        \n",
    "        return Train_DataFrame , Test_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25cbf7ed-db3e-4254-b484-6b1e68af0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devide data to test and train \n",
    "Dataset = PreprocessData.TestAndTrain(Data , 80)\n",
    "\n",
    "TrainData = Dataset[0]\n",
    "TestData = Dataset[1]\n",
    "X = TrainData[['f1','f2','f3','f4','f5','f6','f7']].to_numpy()\n",
    "Xnonbias = X\n",
    "X= np.c_[np.ones((len(X) , 1)) , X]\n",
    "Y = TrainData['label'].to_numpy()\n",
    "\n",
    "xtest =  TestData[['f1','f2','f3','f4','f5','f6','f7']].to_numpy()\n",
    "ytest =  TestData['label'].to_numpy()\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea41bdaa-9900-465a-9f42-df24842dbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData1 = TrainData[TrainData.label == 1]\n",
    "TrainData2 = TrainData[TrainData.label == 2]\n",
    "TrainData3 = TrainData[TrainData.label == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8eea6c-4652-40b2-a514-45b992900c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcff6610-8146-41a2-a686-95b1d5147aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+ np.exp(-1*z))\n",
    "\n",
    "def hypotesis( X , theta):\n",
    "    z = np.dot(X , theta)\n",
    "    yHat = sigmoid(z)\n",
    "    return yHat\n",
    "def error(X , theta , y ,classin):\n",
    "    cost = 0\n",
    "    for i , j in zip(X , y):\n",
    "        x_bar = np.array(np.insert(i, 0, 1))\n",
    "        \n",
    "        y_hat = hypotesis(x_bar, theta)\n",
    "        y_binary = 1.0 if j == classin else 0.0\n",
    "        cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat)\n",
    "    return cost   \n",
    "\n",
    "def train( input_var, label, itrations , classin , alpharate ,theta):\n",
    "    classofInterst = classin\n",
    "    theta1 = theta\n",
    "    y_binary=0\n",
    "    for a in range(itrations):\n",
    "        \n",
    "        \n",
    "        if a % 5000 == 0:\n",
    "            print(f'iteration: {a}')\n",
    "            print(f'cost: {error(input_var,  theta, label , classofInterst)}')\n",
    "            print('--------------------------------------------')\n",
    "            \n",
    "        \n",
    "        for i , xy in enumerate(zip(input_var , label)):\n",
    "            x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "            y_hat = hypotesis(x_bar,theta)\n",
    "            \n",
    "            y_binary = 1.0 if xy[1] == classin else 0.0 \n",
    "             \n",
    "            gradient = (y_binary - y_hat) * x_bar\n",
    "            theta += alpharate * gradient\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "               \n",
    "    return theta\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea724d5-28ea-42a8-81d6-950f0830c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "cost: -116.448726334071\n",
      "--------------------------------------------\n",
      "iteration: 5000\n",
      "cost: -48.41349311701857\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e-2\n",
    "params_0 = np.zeros(Xnonbias.shape[1] + 1)\n",
    "\n",
    "max_iter = 20000\n",
    "\n",
    "\n",
    "a =\\\n",
    "train(Xnonbias/ 8 , Y , 10000 , 1 , alpha , params_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab2456b-3ea4-4882-8c24-f2b4493bef7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1196288107.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [14]\u001b[0;36m\u001b[0m\n\u001b[0;31m    y_binary =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"Class for training and using a model for logistic regression\"\"\"\n",
    "    \n",
    "    def set_values(self, initial_params, alpha=0.01, max_iter=5000, class_of_interest=0):\n",
    "        \"\"\"Set the values for initial params, step size, maximum iteration, and class of interest\"\"\"\n",
    "        self.params = initial_params\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.class_of_interest = class_of_interest\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Sigmoide function\"\"\"\n",
    "        \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, x_bar, params):\n",
    "        \"\"\"predict the probability of a class\"\"\"  \n",
    "\n",
    "        return self._sigmoid(np.dot(params, x_bar))\n",
    "        \n",
    "    \n",
    "    def _compute_cost(self, input_var, output_var, params):\n",
    "        \"\"\"Compute the log likelihood cost\"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "       \n",
    "        for x, y in zip(input_var, output_var):\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, params)\n",
    "            \n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat)\n",
    "            \n",
    "        print(params)   \n",
    "        return cost\n",
    "    \n",
    "    def train(self, input_var, label, print_iter = 5000):\n",
    "        \"\"\"Train the model using batch gradient ascent\"\"\"\n",
    "        \n",
    "        iteration = 1\n",
    "        y_binary =  \n",
    "        while iteration < self.max_iter:\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f'iteration: {iteration}')\n",
    "                print(f'cost: {self._compute_cost(input_var, label, self.params)}')\n",
    "                print(f'grad:{y_binary}')\n",
    "                print('--------------------------------------------')\n",
    "            \n",
    "            for i, xy in enumerate(zip(input_var, label)):\n",
    "                x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "                y_hat = self.predict(x_bar, self.params)\n",
    "                \n",
    "                y_binary = 1.0 if xy[1] == self.class_of_interest else 0.0\n",
    "                gradient = (y_binary - y_hat) * x_bar\n",
    "                self.params += self.alpha * gradient\n",
    "            \n",
    "            iteration +=1\n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    def test(self, input_test, label_test):\n",
    "        \"\"\"Test the accuracy of the model using test data\"\"\"\n",
    "        self.total_classifications = 0\n",
    "        self.correct_classifications = 0\n",
    "        \n",
    "        for x,y in zip(input_test, label_test):\n",
    "            self.total_classifications += 1\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, self.params)\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            \n",
    "            if y_hat >= 0.5 and  y_binary == 1:\n",
    "                # correct classification of class_of_interest\n",
    "                self.correct_classifications += 1\n",
    "              \n",
    "            if y_hat < 0.5 and  y_binary != 1:\n",
    "                # correct classification of an other class\n",
    "                self.correct_classifications += 1\n",
    "                \n",
    "        self.accuracy = self.correct_classifications / self.total_classifications\n",
    "            \n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95c1f90-64c9-43b4-a105-dcb258b82949",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m params_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(Xnonbias\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m digits_regression_model_0 \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m()\n\u001b[1;32m      7\u001b[0m digits_regression_model_0\u001b[38;5;241m.\u001b[39mset_values(params_0, alpha, max_iter, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m digits_regression_model_0\u001b[38;5;241m.\u001b[39mtrain(Xnonbias\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8.0\u001b[39m, Y, \u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# train a classifier for the ZERO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(Xnonbias.shape[1] + 1)\n",
    "\n",
    "max_iter = 80000\n",
    "digits_regression_model_0 = LogisticRegression()\n",
    "digits_regression_model_0.set_values(params_0, alpha, max_iter, 1)\n",
    "\n",
    "\n",
    "digits_regression_model_0.train(Xnonbias/ 8.0, Y, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a81047-419f-4aa5-9333-8e1f3ebc5eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
